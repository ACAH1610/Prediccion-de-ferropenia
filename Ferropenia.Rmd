---
title: "Ferropenia"
author: "Andrea"
date: "2024-03-21"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
  output: null
  html_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE}
#Librerías necesarias
library(dplyr)
library(readxl)
library(ggplot2)
library(GGally)
library(kableExtra)
library(corrplot)
library(patchwork)
library(ggpubr)

library(tm)
library(stringr)
library(class)
library(gmodels)
library(ROCR)
library(caret)
library(kernlab)
```

# 1. Creación del dataset

## 1.1 Importación del excel

Partimos de un excel generado por el Sistema Informático del Laboratorio
(SIL) en el que se puso como condición que los pacientes tuvieran un VCM
por debajo del rango de normalidad, tanto bajo como muy bajo

```{r warning=FALSE, message=FALSE}

ferropenia = read_excel("~/aaaatalasemias/TFM/Ferropenia2.xls", 
col_types = c("text", "text", "numeric","text", "text", "text", "text", "text", "text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "text", "text", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric", "numeric"))
str (ferropenia)

```

## 1.2 Pasos a seguir para conseguir el Dataset objetivo:

### 1.2.1. Excluir a los pacientes que les falte alguna prueba necesaria para filtrar

Necesitaremos los parámetros: Hematíes, hemoglobina, VCM, HCM, ADE, hematocrito, CHCM, PCR, ferritina, IST, además de la edad y sexo del paciente que serán
necesarios para tener en cuenta los valores de referencia.

```{r}
# Definimos las variables necesarias en un vector
pruebas_necesarias = c("HTIE", "HGB", "VCM", "HCM", "IDM", "HTCO", "CHCM", "PCR", "FERR", "ISTR")

# Nos quedamos solo con los pacientes que tengan todas las pruebas
ferropenia_filtrado = ferropenia[complete.cases(ferropenia[, pruebas_necesarias]), ]
```

### 1.2.2. Pasar todas las edades de días (x/365) y meses (x/12) a años

```{r}
# Hacemos una función para que la edad esté en años 
calcular_edad_years=function(numero, letra){
  if (letra == "mes" || letra == "meses") {
    edad_ajustada = numero / 12
  } else if (letra == "días") {
    edad_ajustada = numero / 365
  } else {
    edad_ajustada = numero  # Si no se especifica "mes" o "días", retornar el mismo valor
  }
  
  return(edad_ajustada)
  
}

# creamos una columna nueva con la edad ajustada
ferropenia_filtrado$edad_normalizada <- mapply(calcular_edad_years, 
                                        ferropenia_filtrado$`Edad(numero)`, 
                                        ferropenia_filtrado$`Edad(2parte)`)


```

### 1.2.3. Restringir la edad de los pacientes a >15 días -18 años

Este rango etario se lleva a cabo por ser el rango de edades de los que disponemos bibliografía de rangos de referencia de las variables utilizadas.

```{r}
ferropenia_filtrado = filter(ferropenia_filtrado, edad_normalizada >= (15/365) & edad_normalizada <=18)
```

### 1.2.4. Excluir a los pacientes con una PCR >1,5

Se realiza para descartar una posible inflamación/infección

```{r}
ferropenia_filtrado = filter(ferropenia_filtrado, PCR<1.5)
```

### 1.2.5. Eliminar pacientes duplicados

```{r}
#Seleccionamos los duplicados, quedándonos solo con el primero
duplicados <- duplicated(ferropenia_filtrado$NHC, fromLast = F)

# Filtramos el dataset
ferropenia_sin_duplicados <- ferropenia_filtrado[!duplicados, ]

```

### 1.2.6. Clasificar a los pacientes

Se lleva a cabo la clasificación con las siguientes etiquetas en función de los valores de referencia: 

- Ferropenia latente o funcional: ferritina baja, IST normal o bajo y Hb normal. 

- Anemia ferropénica: ferritina baja y IST bajo, Hb baja. 

- Ausencia ferropenia: ferritina normal, IST normal.

Se añadirán columnas que nos indiquen si cada parámetro está alterado en función de los rangos de referencias por edad y sexo.



```{r}
#Ferritina
#Rangos de referencia:
#0.1666667- 0.5 años= 15,3 – 375
#0.5 – 1 = 13.3-55.8
#1-16= 10.3-55.8
#>16 años (M): 18.7-102
#>16 años (F): 12-150
ferritina_function = function(ferritina, edad, sexo){
  if (edad >= 0.1666667 & edad<0.5 & ferritina< 15.3){
    return (1)
  } else if (edad >= 0.5 & edad<1 & ferritina< 13.3){
    return (1)
  } else if (edad >= 1 & edad<16 & ferritina< 10.3){
    return (1)
  }  else if (edad >= 15 & edad<= 18 & sexo== "M" & ferritina< 18.7)  {
    return (1)
  } else if (edad >= 15 & edad<= 18 & sexo== "F" & ferritina< 12 ) {
    return (1)
  } else {
    return (0)
  }
}
ferropenia_sin_duplicados$ferritina_baja = mapply(ferritina_function, ferropenia_sin_duplicados$FERR, ferropenia_sin_duplicados$edad_normalizada,ferropenia_sin_duplicados$`Sexo del Paciente`)
```

```{r}
#IST
#Rangos de referencia:
#0.1666667-0.25 años : 21 - 63
#0.25-0.4166667 años = 7 - 53
#0.4166667-0.5833333 años = 10 - 43
#0.5833333- 2 años = 6 – 38
#2 años - 12 años: 7 - 43
#12 años - 18 años: 18 - 46


IST_function = function(IST, edad, sexo){
  if (edad >= 0.1666667 & edad<0.25 & IST< 23){
    return(1)
  } else if (edad >= 0.25 & edad<0.4166667 & IST< 7){
    return(1)
  } else if (edad >= 0.4166667 & edad<0.5833333 & IST< 10){
    return (1)
  } else if (edad >= 0.5833333 & edad<2 & IST< 6){
    return (1)
  } else if (edad >= 2 & edad<12 & IST< 7){
    return (1)
  } else if (edad >= 12 & edad<= 18 & IST< 18)  {
    return (1)
  } else {
    return (0)
  }
}
ferropenia_sin_duplicados$IST_bajo = mapply(IST_function, ferropenia_sin_duplicados$ISTR, ferropenia_sin_duplicados$edad_normalizada,ferropenia_sin_duplicados$`Sexo del Paciente`)
```

```{r}
#Hemoglobina 
#Rangos de referencia:
#0.25 – 0.5 años: : 9,5 - 13,5
#0.5 – 2 años:  10,5 - 13,5
#2 - 6 años: 11,5 - 13,5
#6 - 12 años: 11,5 - 15,5
#12 - 18 años (M): 13 - 16
#12 - 18 años (F): 12 – 16
hemoglobina_function = function(hemoglobina, edad, sexo){
   if (edad >=0.1666667 & edad<0.25 & hemoglobina< 9){
    return(1)
  } else if (edad >= 0.25 & edad<0.5 & hemoglobina< 9.5){
    return(1)
  } else if (edad >= 0.5 & edad<2 & hemoglobina< 10.5){
    return (1)
  } else if (edad >= 2 & edad<6 & hemoglobina< 11.5){
    return (1)
  } else if (edad >= 6 & edad<12 & hemoglobina< 11.5){
    return (1)
  } else if (edad >= 12 & edad<= 18 & sexo== "M" & hemoglobina< 13)  {
    return (1)
  } else if (edad >= 12 & edad<= 18 & sexo== "F" & hemoglobina< 12 ) {
    return (1)
  } else {
    return (0)
  }
}
ferropenia_sin_duplicados$Hb_baja = mapply(hemoglobina_function, ferropenia_sin_duplicados$HGB, ferropenia_sin_duplicados$edad_normalizada,ferropenia_sin_duplicados$`Sexo del Paciente`)

```

Hacemos la clasificacion en funcion de los parametros alterados 

- Si ferritina 1 , IST 0 o 1 , hemoglobina 0 = Ferropenia funcional o latente 

- Si ferritina 1 , IST 1, hemoglobina 1 = Anemia ferropénica 

- Si ferritina 0 , IST 0 = Ausencia de ferropenia

```{r}
clasificacion_function = function(ferritina, IST, hemoglobina){
  if (ferritina == 1 & IST==0 & hemoglobina==0){
    return("Ferropenia funcional o latente") #latente
  } else if (ferritina == 1 & IST==1  & hemoglobina==0){
    return ("Ferropenia funcional o latente") #funcional
  } else if (ferritina == 1 & IST==1  & hemoglobina==1){
    return ("Anemia ferropenica")
  } else if (ferritina == 0 & IST==0 ){
    return ("Ausencia de ferropenia")
  }else {
    return ("NA")
}
}
ferropenia_sin_duplicados$Clasificación= mapply(clasificacion_function, ferropenia_sin_duplicados$ferritina_baja, ferropenia_sin_duplicados$IST_bajo , ferropenia_sin_duplicados$Hb_baja)
```


### 1.2.7. Selección de las variables de interés

```{r}
dataset= ferropenia_sin_duplicados[, c("Sexo del Paciente","edad_normalizada", "HTIE", "HGB", "VCM","HCM", "IDM", "CHCM", "HTCO", "Clasificación")]
dataset=filter(dataset, Clasificación != "NA")

names(dataset)[names(dataset) == "edad_normalizada"] = "Edad"
names(dataset)[names(dataset) == "IDM"] = "ADE"
```



```{r}
#Se lleva  a cabo un dataset teniendo en cuenta los reticulocitos
dataset_retis= ferropenia_sin_duplicados[, c("Sexo del Paciente","edad_normalizada", "HTIE", "HGB", "VCM","HCM", "IDM", "CHCM", "HTCO",  "REAB", "RETI", "Clasificación")]
dataset_retis=na.omit(dataset_retis)
dataset_retis=filter(dataset_retis, Clasificación != "NA")

names(dataset_retis)[names(dataset_retis) == "edad_normalizada"] = "Edad"
names(dataset_retis)[names(dataset_retis) == "IDM"] = "ADE"
#colnames(dataset)[colnames(dataset)== "clasificacion"] = "Clasificación"
```

# 2. Análisis descriptivo

## 2.1 Lectura de los datos

Las variables que vamos a utilizar:

```{r}
names(dataset)
```

"clasificacion" contiene las clases a predecir.

El número de observaciones según la clase es:

```{r}
kable(as.data.frame(table(dataset$Clasificación)),
      col.names= c("Clasificación", "Frecuencia"),
      align= "cc")
```


```{r}

# Crear un data frame a partir de la tabla de frecuencias
df = as.data.frame(table(dataset$Clasificación))
names(df) =c("Clasificación", "Frecuencia")
colores = c("red", "grey", "brown")

# Gráfico utilizando ggplot2
ggplot(df, aes(x = Clasificación, y = Frecuencia, fill = Clasificación)) +
  geom_bar(stat = "identity", width = 0.5) +
  scale_fill_manual(values = colores) +
  labs(x = "Clasificación", y = "Frecuencia", title = "Distribución de las clases") +
  theme_minimal()

```

Tenemos una distribución desbalanceada, se tendrá en cuenta para el tratamiento de los datos.

## 2.2 Exploración de los datos

### 2.2.1 Resumen de las variables:

```{r}
summary(dataset)
```

Nuestro dataset no necesita llevar a cabo la eliminación de datos
faltantes (NA) ya que se ha llevado a cabo en la realización del
dataset.

Por último vemos con tipo de variables estamos trabajando:

```{r}
str(dataset)
```

Podemos ver que todas las variables son numéricas excepto el "Sexo del
Paciente" y la "clasificacion", que convertiremos a factor:

```{r}
dataset$`Sexo del Paciente`= as.factor(dataset$`Sexo del Paciente`)
dataset$Clasificación <- factor(dataset$Clasificación, 
                                levels = c("Anemia ferropenica", "Ferropenia funcional o latente", "Ausencia de ferropenia"), 
                                labels = c("AF", "FF/FL", "NF"))

dataset$`Sexo del Paciente`= as.factor(dataset$`Sexo del Paciente`)

str(dataset)
```


```{r}
#En paralelo para el dataset_retis
dataset_retis$`Sexo del Paciente`= as.factor(dataset_retis$`Sexo del Paciente`)
dataset_retis$Clasificación <- factor(dataset_retis$Clasificación, 
                                levels = c("Anemia ferropenica", "Ferropenia funcional o latente", "Ausencia de ferropenia"), 
                                labels = c("AF", "FF/FL", "NF"))
dataset_retis$`Sexo del Paciente` <- factor(dataset_retis$`Sexo del Paciente`, 
                                levels = c("F", "M"), 
                                labels = c(1,0))
```


Podemos ver lo primeros registros:

```{r}
head(dataset)
```

### 2.2.2 Gráficos exploratorios

#### Visualización de las variables cuantitativas en forma de boxplot

```{r}
#seleccionamos solo las variables cuantitativas
variables_cuantitativas <- dataset[, !(names(dataset) %in% c("Sexo del Paciente", "Clasificación"))]
colores= rainbow(ncol(variables_cuantitativas))
boxplot(variables_cuantitativas, col= colores)
```


Como se observa en el gráfico el rango de variabilidad entre variables
es muy grande por eso en algunos casos con rango de valores muy
estrechos queda reducida la caja a una linea. Esto nos indica que será
necesaria la normalización de los datos para que las variables tengan el
mismo peso.


#### Histogramas y boxplot en función de la clase y sexo:

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1= ggplot(dataset, aes(x = Edad, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()


# Gráfico 2: Gráfico de cajas
p2= ggplot(data = dataset, aes(x = Clasificación, y = Edad, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
   
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("Edad", size =15))
final_plot

```

Se observa que hay más frecuencia de pacientes en edades comprendidas entre 0-5 años.
Además, presentan el grupo de Anemia ferropénica se ve más prevalencia en el género femenino a edades más avanzadas, probablemente coincidiendo con la menstruación.


```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1= ggplot(dataset, aes(x = HTIE, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()


# Gráfico 2: Gráfico de cajas
p2= ggplot(data = dataset, aes(x = Clasificación, y = HTIE, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
   
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("Hematíes", size =15))
final_plot
```

El parámetro Hematíes presenta una distribución normal, sin destacar nada acerca de su distribución por clases. 

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1= ggplot(dataset, aes(x = HGB, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()

# Gráfico 2: Gráfico de cajas
p2= ggplot(data = dataset, aes(x = Clasificación, y = HGB, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
   
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("Hemoglobina", size =15))
final_plot
```

El parámetro Hemoglobina presenta una distribución normal, con medidas más bajas en la clase de Anemia ferropénica como cabe esperar.

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1=ggplot(dataset, aes(x = VCM, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()


# Gráfico 2: Gráfico de cajas
p2=ggplot(data = dataset, aes(x = Clasificación, y = VCM, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
  
final_plot <- ggarrange(p1, p2, legend = "top")
final_plot <- annotate_figure(final_plot, top = text_grob("VCM", size =15))
final_plot
```

Distribución del Volumen Corpuscular Medio con cola hacia la izquierda. En cuanto a las clases se puede ver cómo en la anemia ferropénica el VCM marcadamente más bajo.


```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1=ggplot(dataset, aes(x = HCM, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()

# Gráfico 2: Gráfico de cajas
p2=ggplot(data = dataset, aes(x = Clasificación, y = HCM, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
  
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("HCM", size =15))
final_plot
```

La Heglobina Corpuscular Media presenta una distribución casi normal, con valores más bajo en los pacientes con Anemia Ferropénica, ya que los hematíes son hipocrómicos. 

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1=ggplot(dataset, aes(x = ADE, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()

# Gráfico 2: Gráfico de cajas
p2=ggplot(data = dataset, aes(x = Clasificación, y = ADE, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
  
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("ADE", size =15))
final_plot
```

El Ancho de Distribución Eritrocitaria presenta una cola hacia la derecha, presentando valores más altos en Anemia Ferropénica que se corresponde a la anisocitosis que presentan los pacientes.

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1=ggplot(dataset, aes(x = CHCM, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()

# Gráfico 2: Gráfico de cajas
p2=ggplot(data = dataset, aes(x = Clasificación, y = CHCM, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
  
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("CHCM", size =15))
final_plot
```

La concentración de hemoglobina corpuscular media presenta una distribución normal, con valores más bajos en anemia ferropénica.

```{r warning=FALSE, message=FALSE}
# Gráfico 1: Histograma
p1=ggplot(dataset, aes(x = HTCO, color = `Sexo del Paciente`)) +
    geom_histogram(fill = "white", alpha = 0.5, position = "identity") +
    theme_classic()

# Gráfico 2: Gráfico de cajas
p2=ggplot(data = dataset, aes(x = Clasificación, y = HTCO, col = `Sexo del Paciente`)) + 
    geom_boxplot()+
    theme_classic()
  
final_plot = ggarrange(p1, p2, legend = "top")
final_plot = annotate_figure(final_plot, top = text_grob("Hto", size =15))
final_plot
```

El hematocrito presenta una distribución normal y en la clase de anemia ferropénica presenta valores más bajos.

```{r}
#Asignamos el valor 1 al sexo femenino y 0 al género masculino
dataset$`Sexo del Paciente` <- factor(dataset$`Sexo del Paciente`, 
                                levels = c("F", "M"), 
                                labels = c(1,0))
```


# 3. Preprocesamiento de datos

## 3.1 División del dataset en train y test

Hacemos la partición del dataset: 70% de los pacientes para el entrenamiento y el 30% para el testeo. 

```{r}
#Dejamos las variables categóricas como variables dummy
dummy <- model.matrix(~ `Sexo del Paciente` - 1, data = dataset)
dataset <- as.data.frame(cbind(dummy, dataset[2:ncol(dataset)]))
colnames(dataset)[colnames(dataset)== "`Sexo del Paciente`1"] = "Sexo_1"
colnames(dataset)[colnames(dataset)== "`Sexo del Paciente`0"] = "Sexo_0"
str(dataset)
```
```{r}
# para dataset_retis

dummy <- model.matrix(~ `Sexo del Paciente` - 1, data = dataset_retis)
dataset_retis <- as.data.frame(cbind(dummy, dataset_retis[2:ncol(dataset_retis)]))
colnames(dataset_retis)[colnames(dataset_retis)== "`Sexo del Paciente`1"] = "Sexo_1"
colnames(dataset_retis)[colnames(dataset_retis)== "`Sexo del Paciente`0"] = "Sexo_0"
```



```{r}
n = nrow(dataset)
# Separamos los datos en Train y Test aleatoriamente
set.seed(29)
train = createDataPartition(dataset$Clasificación,p = 0.7, list = FALSE)
dataset.train = dataset[train,] #70%
dataset.test  = dataset[-train,] #30%

x.train= dataset.train[1:(ncol(dataset.train)-1)]
y.train= dataset.train[ncol(dataset.train)]
x.test= dataset.test[1:(ncol(dataset.train)-1)]
y.test= dataset.test[ncol(dataset.train)]
```


```{r}
# para dataset_retis
n = nrow(dataset_retis)
# Separamos los datos en Train y Test aleatoriamente
set.seed(29)
train = createDataPartition(dataset_retis$Clasificación,p = 0.7, list = FALSE)
dataset.train_retis = dataset_retis[train,] #70%
dataset.test_retis  = dataset_retis[-train,] #30%

x.train_retis= dataset.train_retis[1:(ncol(dataset.train_retis)-1)]
y.train_retis= dataset.train_retis[ncol(dataset.train_retis)]
x.test_retis= dataset.test_retis[1:(ncol(dataset.test_retis)-1)]
y.test_retis= dataset.test_retis[ncol(dataset.test_retis)]
```


Comprobamos que la los casos están distribuidos equitativamente entre el dataset train y test
```{r}
porcentajes_train=round(table(dataset.train$Clasificación)/nrow(dataset.train)*100,2)
porcentajes_test=round(table(dataset.test$Clasificación)/nrow(dataset.test)*100,2)
```

```{r}
kable(rbind(porcentajes_train,porcentajes_test))
```


Efectivamente, las clases se encuentran proporcionados en cada partición.

## 3.2 Prueba de los diferentes modelos - modelos basales

```{r}
# Normalización de las variables para los métodos que lo requieren:

preProcValues = preProcess(x.train, method = c("center", "scale"))

trainTransformed_basal = cbind(predict(preProcValues, x.train), y.train)
testTransformed_basal = cbind(predict(preProcValues, x.test), y.test)

```

Especificamos el método de validación que se van a utilizar en todos los modelos del paquete caret:
```{r}
set.seed(29)
fitControl <- trainControl(method = "cv",
                           number = 10,
                           summaryFunction = multiClassSummary
                           )
```

### 3.2.1 k-Nearest Neighbour 

Para ello utilizamos los datos normalizados
```{r warning=FALSE, message=FALSE}
# modelo
set.seed(29)
modelo_knn_basal <- train(Clasificación ~ ., data = trainTransformed_basal,
                    method = "knn",
                    metric = "Kappa",
                    trControl= fitControl)

modelo_knn_basal

# Predicciones
p <- predict(modelo_knn_basal, newdata = testTransformed_basal[,-(ncol(testTransformed_basal))])

# Evaluación
(mconfusion_knn_basal <- confusionMatrix(p, testTransformed_basal$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_knn_basal <- data.frame(
  Modelo              = "KNN_basal",
  Precisión           = round((ifelse(is.na(mconfusion_knn_basal$byClass[1,5]), 0, mconfusion_knn_basal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[2,5]), 0, mconfusion_knn_basal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[3,5]), 0, mconfusion_knn_basal$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_knn_basal$byClass[1,1]), 0, mconfusion_knn_basal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[2,1]), 0, mconfusion_knn_basal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[3,1]), 0, mconfusion_knn_basal$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_knn_basal$byClass[1,2]), 0, mconfusion_knn_basal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[2,2]), 0, mconfusion_knn_basal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[3,2]), 0, mconfusion_knn_basal$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_knn_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_knn_basal$byClass[1,7]), 0, mconfusion_knn_basal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[2,7]), 0, mconfusion_knn_basal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_knn_basal$byClass[3,7]), 0, mconfusion_knn_basal$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_knn_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_knn_basal

```

### 3.2.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_bayes_basal = train(Clasificación ~ ., data = dataset.train,
                   method = "nb",
                   metric = "Kappa",
                    trControl= fitControl))
  
# Predicciones
p <- predict(modelo_bayes_basal, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_Bayes_basal <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_NaiveBayes_basal <- data.frame(
  Modelo              = "Bayes_basal",
  Precisión           = round((ifelse(is.na(mconfusion_Bayes_basal$byClass[1,5]), 0, mconfusion_Bayes_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[2,5]), 0, mconfusion_Bayes_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[3,5]), 0, mconfusion_Bayes_basal$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bayes_basal$byClass[1,1]), 0, mconfusion_Bayes_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[2,1]), 0, mconfusion_Bayes_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[3,1]), 0, mconfusion_Bayes_basal$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bayes_basal$byClass[1,2]), 0, mconfusion_Bayes_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[2,2]), 0, mconfusion_Bayes_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[3,2]), 0, mconfusion_Bayes_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bayes_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bayes_basal$byClass[1,7]), 0, mconfusion_Bayes_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[2,7]), 0, mconfusion_Bayes_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_Bayes_basal$byClass[3,7]), 0, mconfusion_Bayes_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bayes_basal[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes_basal

```


### 3.2.4 ANN
```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)

(modelo_ann_basal <- train(Clasificación ~ ., data = trainTransformed_basal,
                    method = "nnet",
                    metric = "Kappa",
                    trControl= fitControl,
                    trace = FALSE))


# Predicciones
p <- predict(modelo_ann_basal, newdata = testTransformed_basal[1:(ncol(testTransformed_basal)-1)])

# Evaluación
res = table(p, testTransformed_basal$Clasificación)
(mconfusion_ann_basal = confusionMatrix(res, mode="everything", positive="AF"))
```

```{r}
resultados_ann_basal <- data.frame(
  Modelo              = "ANN_basal",
  Precisión           = round((ifelse(is.na(mconfusion_ann_basal$byClass[1,5]), 0, mconfusion_ann_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[2,5]), 0, mconfusion_ann_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[3,5]), 0, mconfusion_ann_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_ann_basal$byClass[1,1]), 0, mconfusion_ann_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[2,1]), 0, mconfusion_ann_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[3,1]), 0, mconfusion_ann_basal$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_ann_basal$byClass[1,2]), 0, mconfusion_ann_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[2,2]), 0, mconfusion_ann_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[3,2]), 0, mconfusion_ann_basal$byClass[3,2])  ) / 3 , 3),
  kappa               = round(mconfusion_ann_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_ann_basal$byClass[1,7]), 0, mconfusion_ann_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[2,7]), 0, mconfusion_ann_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_ann_basal$byClass[3,7]), 0, mconfusion_ann_basal$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_ann_basal[["overall"]][["Accuracy"]], 3)
)

resultados_ann_basal

```


### 3.2.5 Support Vector Machine

#### 3.2.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal_basal = train(Clasificación ~ ., data = trainTransformed_basal,
                          method = "svmLinear",
                          metric = "Kappa",
                    trControl= fitControl
                          ))

# Predicciones
p = predict(modelo_svmlineal_basal, newdata = testTransformed_basal[1:(ncol(testTransformed_basal)-1)])

# Evaluación
(mconfusion_svmlineal_basal = confusionMatrix(p, testTransformed_basal$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal_basal <- data.frame(
  Modelo              = "SVM lineal_basal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal_basal$byClass[1,5]), 0, mconfusion_svmlineal_basal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[2,5]), 0, mconfusion_svmlineal_basal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[3,5]), 0, mconfusion_svmlineal_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal_basal$byClass[1,1]), 0, mconfusion_svmlineal_basal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[2,1]), 0, mconfusion_svmlineal_basal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[3,1]), 0, mconfusion_svmlineal_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal_basal$byClass[1,2]), 0, mconfusion_svmlineal_basal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[2,2]), 0, mconfusion_svmlineal_basal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[3,2]), 0, mconfusion_svmlineal_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_svmlineal_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmlineal_basal$byClass[1,7]), 0, mconfusion_svmlineal_basal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[2,7]), 0, mconfusion_svmlineal_basal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal_basal$byClass[3,7]), 0, mconfusion_svmlineal_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_svmlineal_basal

```

 
#### 3.2.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)
(modelo_svmradial_basal = train(Clasificación ~ ., data = trainTransformed_basal,
                          method = "svmRadial",
                         metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmradial_basal, newdata = testTransformed_basal[1:(ncol(testTransformed_basal)-1)])

# Evaluación
(mconfusion_svmradial_basal = confusionMatrix(p, testTransformed_basal$Clasificación, mode="everything", positive="AF"))

```



```{r}
resultados_SVM_radial_basal <- data.frame(
  Modelo              = "SVM-Radial_basal",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial_basal$byClass[1,5]), 0, mconfusion_svmradial_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[2,5]), 0, mconfusion_svmradial_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[3,5]), 0, mconfusion_svmradial_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial_basal$byClass[1,1]), 0, mconfusion_svmradial_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[2,1]), 0, mconfusion_svmradial_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[3,1]), 0, mconfusion_svmradial_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial_basal$byClass[1,2]), 0, mconfusion_svmradial_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[2,2]), 0, mconfusion_svmradial_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[3,2]), 0, mconfusion_svmradial_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_svmradial_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial_basal$byClass[1,7]), 0, mconfusion_svmradial_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[2,7]), 0, mconfusion_svmradial_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_svmradial_basal$byClass[3,7]), 0, mconfusion_svmradial_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_SVM_radial_basal

```


#### 3.2.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly_basal = train(Clasificación ~ ., data = trainTransformed_basal,
                          method = "svmPoly",
                          metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmpoly_basal, newdata = testTransformed_basal[1:(ncol(testTransformed_basal)-1)])

# Evaluación
(mconfusion_svmpoly_basal = confusionMatrix(p, testTransformed_basal$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_SVM_poly_basal <- data.frame(
  Modelo              = "SVM-poly_basal",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly_basal$byClass[1,5]), 0, mconfusion_svmpoly_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[2,5]), 0, mconfusion_svmpoly_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[3,5]), 0, mconfusion_svmpoly_basal$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly_basal$byClass[1,1]), 0, mconfusion_svmpoly_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[2,1]), 0, mconfusion_svmpoly_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[3,1]), 0, mconfusion_svmpoly_basal$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly_basal$byClass[1,2]), 0, mconfusion_svmpoly_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[2,2]), 0, mconfusion_svmpoly_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[3,2]), 0, mconfusion_svmpoly_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_svmpoly_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly_basal$byClass[1,7]), 0, mconfusion_svmpoly_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[2,7]), 0, mconfusion_svmpoly_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_svmpoly_basal$byClass[3,7]), 0, mconfusion_svmpoly_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly_basal[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly_basal

```


### 3.2.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_C5.0_basal<- train(Clasificación ~ ., data = dataset.train,
                     method = "C5.0",
                     metric = "Kappa",
                    trControl= fitControl
                     ))

# Predicciones
p <- predict(modelo_C5.0_basal, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_arbol_basal = confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_arbol_basal <- data.frame(
  Modelo              = "Árbol_basal",
  Precisión           = round((ifelse(is.na(mconfusion_arbol_basal$byClass[1,5]), 0, mconfusion_arbol_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[2,5]), 0, mconfusion_arbol_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[3,5]), 0, mconfusion_arbol_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol_basal$byClass[1,1]), 0, mconfusion_arbol_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[2,1]), 0, mconfusion_arbol_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[3,1]), 0, mconfusion_arbol_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_arbol_basal$byClass[1,2]), 0, mconfusion_arbol_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[2,2]), 0, mconfusion_arbol_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[3,2]), 0, mconfusion_arbol_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_arbol_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol_basal$byClass[1,7]), 0, mconfusion_arbol_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[2,7]), 0, mconfusion_arbol_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_arbol_basal$byClass[3,7]), 0, mconfusion_arbol_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_arbol_basal

```


### 3.2.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_forest_basal <- train(Clasificación ~ ., data = dataset.train,
                   method = "ranger",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_forest_basal, newdata = dataset.test)


# Evaluación
(mconfusion_forest_basal <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF_basal <- data.frame(
  Modelo              = "Random Forest_basal",
  Precisión           = round((ifelse(is.na(mconfusion_forest_basal$byClass[1,5]), 0, mconfusion_forest_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[2,5]), 0, mconfusion_forest_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[3,5]), 0, mconfusion_forest_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest_basal$byClass[1,1]), 0, mconfusion_forest_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[2,1]), 0, mconfusion_forest_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[3,1]), 0, mconfusion_forest_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest_basal$byClass[1,2]), 0, mconfusion_forest_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[2,2]), 0, mconfusion_forest_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[3,2]), 0, mconfusion_forest_basal$byClass[3,2])  ) /3, 3),
  kappa               = round(mconfusion_forest_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest_basal$byClass[1,7]), 0, mconfusion_forest_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[2,7]), 0, mconfusion_forest_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_forest_basal$byClass[3,7]), 0, mconfusion_forest_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_RF_basal

```


### 3.2.8 Boosting

```{r message=FALSE}
# Ajuste del modelo
set.seed(29)
modelo_Gboosting <- train(Clasificación ~ ., data = dataset.train,
                   method = "gbm",
                   metric = "Kappa",
                    trControl= fitControl
                   )
```


```{r}
modelo_Gboosting
# Predicciones
p <- predict(modelo_Gboosting, newdata = dataset.test)


# Evaluación
(mconfusion_boosting_basal <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_boosting_basal <- data.frame(
  Modelo              = "Boosting_basal",
  Precisión           = round((ifelse(is.na(mconfusion_boosting_basal$byClass[1,5]), 0, mconfusion_boosting_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[2,5]), 0, mconfusion_boosting_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[3,5]), 0, mconfusion_boosting_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting_basal$byClass[1,1]), 0, mconfusion_boosting_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[2,1]), 0, mconfusion_boosting_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[3,1]), 0, mconfusion_boosting_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting_basal$byClass[1,2]), 0, mconfusion_boosting_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[2,2]), 0, mconfusion_boosting_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[3,2]), 0, mconfusion_boosting_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_boosting_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting_basal$byClass[1,7]), 0, mconfusion_boosting_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[2,7]), 0, mconfusion_boosting_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_boosting_basal$byClass[3,7]), 0, mconfusion_boosting_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_boosting_basal

```

### 3.2.9 Bagging

```{r}
# Ajuste del modelo
set.seed(29)
(modelo_Bagging_basal <- train(Clasificación ~ ., data = dataset.train,
                   method = "parRF",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_Bagging_basal, newdata = dataset.test)


# Evaluación
(mconfusion_Bagging_basal<- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_bagging_basal <- data.frame(
  Modelo              = "Bagging_basal",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging_basal$byClass[1,5]), 0, mconfusion_Bagging_basal$byClass[1,5]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[2,5]), 0, mconfusion_Bagging_basal$byClass[2,5]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[3,5]), 0, mconfusion_Bagging_basal$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging_basal$byClass[1,1]), 0, mconfusion_Bagging_basal$byClass[1,1]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[2,1]), 0, mconfusion_Bagging_basal$byClass[2,1]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[3,1]), 0, mconfusion_Bagging_basal$byClass[3,1])  ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging_basal$byClass[1,2]), 0, mconfusion_Bagging_basal$byClass[1,2]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[2,2]), 0, mconfusion_Bagging_basal$byClass[2,2]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[3,2]), 0, mconfusion_Bagging_basal$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging_basal[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging_basal$byClass[1,7]), 0, mconfusion_Bagging_basal$byClass[1,7]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[2,7]), 0, mconfusion_Bagging_basal$byClass[2,7]) +
                               ifelse(is.na(mconfusion_Bagging_basal$byClass[3,7]), 0, mconfusion_Bagging_basal$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging_basal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging_basal[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_bagging_basal

```

### 3.2.10 Tabla resumen del rendimiento de los distintos modelos basales

```{r}

tabla_resumen_basal= rbind(resultados_knn_basal,
                     resultados_NaiveBayes_basal,
                     resultados_ann_basal,
                     resultados_SVM_radial_basal,
                     resultados_svmlineal_basal,
                     resultados_SVM_poly_basal,
                     resultados_arbol_basal, 
                     resultados_RF_basal,
                     resultados_boosting_basal,
                     resultados_bagging_basal
                     )

#Establecemos el orden por valores de Accuracy
orden = order(tabla_resumen_basal$Sensibilidad, decreasing = TRUE)
tabla_resumen_ordenada_basal <- tabla_resumen_basal[orden, ]

kable(tabla_resumen_ordenada_basal, digits = 3, caption = "Rendimiento de los modelos")
```

## 3.3 Eliminación de correlaciones altas

Vemos también la matriz de correlación a para ver si hay relación entre los parámetros:
```{r}
dataset.train_cor= dataset.train
dataset.train_cor$Clasificación= as.numeric(dataset.train_cor$Clasificación)
```

```{r}
cor(dataset.train_cor[2:ncol(dataset.train_cor)])
```

```{r}
corrplot(cor(dataset.train_cor[2:ncol(dataset.train_cor)],method="spearman"))
```

```{r}
ggpairs(dataset.train_cor[2:ncol(dataset.train)])
```

Se ve que VCM y HCM una correlación elevada con un índice de correlación de 0.918 al igual que el Hto y la hemoglobina con índice de correlación del 0.930.
Se decide eliminar las variables con índice mayor de 0.80.


```{r}
descrCor <-  cor(x.train, method="spearman")
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)
summary(descrCor[upper.tri(descrCor)])

#Eliminamos las correlaciones >0.80
highlyCorDescr <- findCorrelation(descrCor, cutoff = .90)
x.train_filtered <- x.train[,-highlyCorDescr]
descrCor2 <- cor(x.train_filtered, method="spearman")
summary(descrCor2[upper.tri(descrCor2)])

```
Resultado es la eliminación de 3 variables: uno de los sexos (estaba repetido), Hb y HCM.


```{r}
# Para dataset_retis
descrCor <-  cor(x.train_retis, method="spearman")
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)
summary(descrCor[upper.tri(descrCor)])

#Eliminamos las correlaciones >0.80
highlyCorDescr <- findCorrelation(descrCor, cutoff = .90)
x.train_filtered_retis <- x.train_retis[,-highlyCorDescr]
descrCor2 <- cor(x.train_filtered_retis, method="spearman")
summary(descrCor2[upper.tri(descrCor2)])

```

```{r}
#Quitamos estas variables también del dataset.test
x.test_filtered= x.test[, colnames(x.train_filtered)]


#dataset_retis
x.test_filtered_retis= x.test_retis[, colnames(x.train_filtered_retis)]
```

```{r}
#Cambio de nombre de la columna Sexo
names(x.train_filtered)[names(x.train_filtered) == "Sexo_1"] = "Sexo"
names(x.train_filtered_retis)[names(x.train_filtered_retis) == "Sexo_1"] = "Sexo"

names(x.test_filtered)[names(x.test_filtered) == "Sexo_1"] = "Sexo"
names(x.test_filtered_retis)[names(x.test_filtered_retis) == "Sexo_1"] = "Sexo"
```


## 3.4 Análisis de componentes principales

Por último, se puede realizar una representación en dimensión reducida:
```{r}
PCA = prcomp(dataset.train[2:(ncol(dataset.train)-1)],center = TRUE,scale. = TRUE)

# Obtener las varianzas explicadas por cada componente principal
varianza_explicada = PCA$sdev^2

# Calcular el porcentaje de variabilidad explicada por PC1 y PC2
porcentaje_explicado_PC1 <- varianza_explicada[1] / sum(varianza_explicada) * 100
porcentaje_explicado_PC2 <- varianza_explicada[2] / sum(varianza_explicada) * 100


PCA.raw = PCA$x[,1:2]
plot(PCA.raw[,1], PCA.raw[,2],main = "Componentes principales",xlab = paste("PC1:", round(porcentaje_explicado_PC1, 2), "%"),ylab = paste("PC2:", round(porcentaje_explicado_PC2, 2), "%"),type="n")
points(PCA.raw[,1], PCA.raw[,2],col=as.factor(dataset.train$Clasificación))
```

No se ven grupos según la clasificación.

## 3.6 Normalizamos 


Realizamos la normalización de los datos basándonos en los datos de entrenamiento y será esta normalización la que utilicemos para la partición test.

```{r}
preProcValues = preProcess(x.train_filtered, method = c("center", "scale"))

trainTransformed = predict(preProcValues, x.train_filtered)
testTransformed = predict(preProcValues, x.test_filtered)
dataset.test=cbind(testTransformed, y.test)
dataset.train= cbind(trainTransformed, y.train)

```

```{r}
#dataset_retis

preProcValues_retis = preProcess(x.train_filtered_retis, method = c("center", "scale"))

trainTransformed = predict(preProcValues_retis, x.train_filtered_retis)
testTransformed = predict(preProcValues_retis, x.test_filtered_retis)
dataset.test_retis=cbind(testTransformed, y.test_retis)
dataset.train_retis= cbind(trainTransformed, y.train_retis)

names(dataset.train_retis)[names(dataset.train_retis) == "Sexo_1"] = "Sexo"
names(dataset.test_retis)[names(dataset.test_retis) == "Sexo_1"] = "Sexo"
```


Vemos el resultado de la normalización:
```{r}
summary(trainTransformed)
#Volvemos a representar el boxplot de las variables cuantitativas del conjunto de entrenamiento
boxplot(trainTransformed, col= colores)
```

```{r}
names(dataset.train)
```



## 3.6 Balanceo de los datos de entrenamiento

Como hemos visto previamente, tenemos unos datos desbalanceados, por lo que vamos a utilizar la función SMOTE que crea nuevos casos sintéticos basados en los casos minoritarios.


```{r  warning=FALSE, message=FALSE}

dataset.train_SMOTE=dataset.train
library(smotefamily)
#oversampling AF
set.seed(29)
for (i in 1:nrow(dataset.train_SMOTE)){
  dataset.train_SMOTE$AF[i] <- ifelse(dataset.train_SMOTE$Clasificación[i] == "AF","AF",0)
}
TRAINSET123.2 <- dataset.train_SMOTE[,-(ncol(dataset.train_SMOTE)-1)] 
smote_result_AF = SMOTE(TRAINSET123.2[,-ncol(TRAINSET123.2)],target = TRAINSET123.2$AF, K = 18, dup_size = 1)

oversampled_AF = smote_result_AF$data
str(oversampled_AF)

extra_AF<- filter(oversampled_AF, oversampled_AF$class == "AF")
nrow(extra_AF)
length(extra_AF)
str(extra_AF)

#oversampling FF/FL  
for (i in 1:nrow(dataset.train_SMOTE)){
  dataset.train_SMOTE$FF[i] <- ifelse(dataset.train_SMOTE$Clasificación[i] == "FF/FL","FF/FL",0)
}
TRAINSET123.3 <- dataset.train_SMOTE[,-c(ncol(dataset.train_SMOTE)-1, (ncol(dataset.train_SMOTE)-2))] 
smote_result_FF = SMOTE(TRAINSET123.3[,-ncol(TRAINSET123.3)],target = TRAINSET123.3$FF, K = 18, dup_size = 1)

oversampled_FF = smote_result_FF$data
extra_FF_FL<- filter(oversampled_FF, oversampled_FF$class == "FF/FL")
str(extra_FF_FL)

#create NEWTRAIN SET 
data.train= subset(dataset.train_SMOTE, Clasificación == "NF")
#Nos quedamos con 200 pacientes de NF
#data.train= data.train[sample(nrow(data.train), 200),]

data.train= data.train[,-c(ncol(data.train), (ncol(data.train)-1))]

#Cambiamos el nombre a la columna de clasificación para poder fusionarlas
colnames(extra_AF)[colnames(extra_AF) == "class"]="Clasificación"
colnames(extra_FF_FL)[colnames(extra_FF_FL) == "class"] = "Clasificación"


data.train2 <- rbind(data.train,extra_AF, extra_FF_FL)
dataset.train_SMOTE=data.train2

# Aletorizamos el orden
dataset.train_SMOTE=dataset.train_SMOTE[sample(nrow(dataset.train_SMOTE)), ]
```

```{r}
#vemos cómo queda la distribución
table(dataset.train$Clasificación)
table(dataset.train_SMOTE$Clasificación)
round(table(dataset.train_SMOTE$Clasificación)/nrow(dataset.train_SMOTE)*100,2)
```
```{r}
#repetimos el proceso para tener en cuenta los reticulocitos
dataset.train_SMOTE_retis=dataset.train_retis
library(smotefamily)
#oversampling AF
set.seed(29)
for (i in 1:nrow(dataset.train_SMOTE_retis)){
  dataset.train_SMOTE_retis$AF[i] <- ifelse(dataset.train_SMOTE_retis$Clasificación[i] == "AF","AF",0)
}
TRAINSET123.2 <- dataset.train_SMOTE_retis[,-(ncol(dataset.train_SMOTE_retis)-1)] 
smote_result_AF = SMOTE(TRAINSET123.2[,-ncol(TRAINSET123.2)],target = TRAINSET123.2$AF, K = 18, dup_size = 1)

oversampled_AF = smote_result_AF$data
str(oversampled_AF)

extra_AF<- filter(oversampled_AF, oversampled_AF$class == "AF")
nrow(extra_AF)
length(extra_AF)
str(extra_AF)

#oversampling FF/FL  
for (i in 1:nrow(dataset.train_SMOTE_retis)){
  dataset.train_SMOTE_retis$FF[i] <- ifelse(dataset.train_SMOTE_retis$Clasificación[i] == "FF/FL","FF/FL",0)
}
TRAINSET123.3 <- dataset.train_SMOTE_retis[,-c(ncol(dataset.train_SMOTE_retis)-1, (ncol(dataset.train_SMOTE_retis)-2))] 
smote_result_FF = SMOTE(TRAINSET123.3[,-ncol(TRAINSET123.3)],target = TRAINSET123.3$FF, K = 18, dup_size = 1)

oversampled_FF = smote_result_FF$data
extra_FF_FL<- filter(oversampled_FF, oversampled_FF$class == "FF/FL")
str(extra_FF_FL)

#create NEWTRAIN SET 
data.train= subset(dataset.train_SMOTE_retis, Clasificación == "NF")

data.train= data.train[,-c(ncol(data.train), (ncol(data.train)-1))]

#Cambiamos el nombre a la columna de clasificación para poder fusionarlas
colnames(extra_AF)[colnames(extra_AF) == "class"]="Clasificación"
colnames(extra_FF_FL)[colnames(extra_FF_FL) == "class"] = "Clasificación"



data.train2 <- rbind(data.train,extra_AF, extra_FF_FL)
table(data.train2$Clasificación)
dataset.train_SMOTE_retis=data.train2

# Aletorizamos el orden
dataset.train_SMOTE_retis=dataset.train_SMOTE_retis[sample(nrow(dataset.train_SMOTE_retis)), ]
```


# 4. Aplicación de cada algoritmo para la clasificación 

## 4.1. Clases equilibradas

### 4.1.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                    method = "knn",
                    metric = "Kappa",
                    trControl= fitControl)

modelo_knn

# Predicciones
p <- predict(modelo_knn, newdata = dataset.test[,-(ncol(dataset.test))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn


```




### 4.1.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "nb",
                   metric = "Kappa",
                   trControl= fitControl))
  
# Predicciones
p <- predict(modelo_bayes, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes

```


### 4.1.4 ANN
```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                    method = "nnet",
                    metric = "Kappa",
                    trControl= fitControl,
                    trace = FALSE))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
res = table(p, dataset.test$Clasificación)
(mconfusion_ann = confusionMatrix(res, mode="everything", positive="AF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN caret",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann


```

### 4.1.5 Support Vector Machine

#### 4.1.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmLinear",
                          metric = "Kappa",
                          trControl= fitControl
                          ))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal

```

 
#### 4.1.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmRadial",
                         metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_SVM_radial <- data.frame(
  Modelo              = "SVM-Radial",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial$byClass[1,5]), 0, mconfusion_svmradial$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,5]), 0, mconfusion_svmradial$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,5]), 0, mconfusion_svmradial$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial$byClass[1,1]), 0, mconfusion_svmradial$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,1]), 0, mconfusion_svmradial$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,1]), 0, mconfusion_svmradial$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial$byClass[1,2]), 0, mconfusion_svmradial$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,2]), 0, mconfusion_svmradial$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,2]), 0, mconfusion_svmradial$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svmradial[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial$byClass[1,7]), 0, mconfusion_svmradial$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,7]), 0, mconfusion_svmradial$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,7]), 0, mconfusion_svmradial$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_radial

```

#### 4.1.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmPoly",
                          metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly

```



### 4.1.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                     method = "C5.0",
                     metric = "Kappa",
                    trControl= fitControl
                     ))

# Predicciones
p <- predict(modelo_C5.0, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol

```

### 4.1.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "ranger",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF

```


### 4.1.8 Boosting

```{r message=FALSE}
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "gbm",
                  metric = "Kappa",
                    trControl= fitControl
                   )
```


```{r}
modelo_boosting
# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_boosting <- data.frame(
  Modelo              = "Boosting",
  Precisión           = round((ifelse(is.na(mconfusion_boosting$byClass[1,5]), 0, mconfusion_boosting$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,5]), 0, mconfusion_boosting$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,5]), 0, mconfusion_boosting$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting$byClass[1,1]), 0, mconfusion_boosting$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,1]), 0, mconfusion_boosting$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,1]), 0, mconfusion_boosting$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting$byClass[1,2]), 0, mconfusion_boosting$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,2]), 0, mconfusion_boosting$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,2]), 0, mconfusion_boosting$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_boosting[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting$byClass[1,7]), 0, mconfusion_boosting$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,7]), 0, mconfusion_boosting$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,7]), 0, mconfusion_boosting$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting[["overall"]][["Accuracy"]], 3)
)

resultados_boosting

```


### 4.1.9 Bagging

```{r}
# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "parRF",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging


```

### 4.1.10 Tabla resumen del rendimiento de los distintos modelos equilibrados

```{r}

tabla_resumen_equilibrado = rbind(
                    resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Accuracy
orden = order(tabla_resumen_equilibrado$Sensibilidad, decreasing = TRUE)
tabla_resumen_equilibrado_ordenada <- tabla_resumen_equilibrado[orden, ]
tabla_final=tabla_resumen_equilibrado_ordenada
kable(tabla_resumen_equilibrado_ordenada, digits = 3, caption = "Rendimiento de los modelos")
```


## 4.2. Clases DESequilibradas

### 4.2.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train,
                    method = "knn",
                   metric = "Kappa",
                    trControl= fitControl)

modelo_knn

# Predicciones
p <- predict(modelo_knn, newdata = dataset.test[,-(ncol(dataset.test))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn

```

### 4.2.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train,
                   method = "nb",
                   metric = "Kappa",
                    trControl= fitControl))
  
# Predicciones
p <- predict(modelo_bayes, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes
```

### 4.2.4 ANN
```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train,
                    method = "nnet",
                    metric = "Kappa",
                    trControl= fitControl,
                    trace = FALSE))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_ann = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann

```

### 4.2.5 Support Vector Machine

#### 4.2.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train,
                          method = "svmLinear",
                          metric = "Kappa",
                    trControl= fitControl
                          ))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal
```
 
#### 4.2.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train,
                          method = "svmRadial",
                        metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```



```{r}
resultados_SVM_radial <- data.frame(
  Modelo              = "SVM-Radial",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial$byClass[1,5]), 0, mconfusion_svmradial$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,5]), 0, mconfusion_svmradial$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,5]), 0, mconfusion_svmradial$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial$byClass[1,1]), 0, mconfusion_svmradial$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,1]), 0, mconfusion_svmradial$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,1]), 0, mconfusion_svmradial$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial$byClass[1,2]), 0, mconfusion_svmradial$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,2]), 0, mconfusion_svmradial$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,2]), 0, mconfusion_svmradial$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svmradial[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial$byClass[1,7]), 0, mconfusion_svmradial$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,7]), 0, mconfusion_svmradial$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,7]), 0, mconfusion_svmradial$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_radial
```


#### 4.2.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train,
                          method = "svmPoly",
                          metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

# Mostrar resultados
resultados_SVM_poly

```




### 4.2.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train,
                     method = "C5.0",
                     metric = "Kappa",
                    trControl= fitControl
                     ))

# Predicciones
p <- predict(modelo_C5.0, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol
```



### 4.2.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train,
                   method = "ranger",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF

```

### 4.2.8 Boosting

```{r message=FALSE}
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train,
                   method = "gbm",
                   metric = "Kappa",
                    trControl= fitControl
                   )
```


```{r}
# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_boosting <- data.frame(
  Modelo              = "Boosting",
  Precisión           = round((ifelse(is.na(mconfusion_boosting$byClass[1,5]), 0, mconfusion_boosting$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,5]), 0, mconfusion_boosting$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,5]), 0, mconfusion_boosting$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting$byClass[1,1]), 0, mconfusion_boosting$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,1]), 0, mconfusion_boosting$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,1]), 0, mconfusion_boosting$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting$byClass[1,2]), 0, mconfusion_boosting$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,2]), 0, mconfusion_boosting$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,2]), 0, mconfusion_boosting$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_boosting[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting$byClass[1,7]), 0, mconfusion_boosting$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,7]), 0, mconfusion_boosting$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,7]), 0, mconfusion_boosting$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting[["overall"]][["Accuracy"]], 3)
)

resultados_boosting

```


### 4.2.9 Bagging

```{r}
# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train,
                   method = "parRF",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging

```


### 4.2.10 Tabla resumen del rendimiento de los distintos modelos DESequilibrados

```{r}

tabla_resumen_desequilibrada = rbind(resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Accuracy
orden = order(tabla_resumen_desequilibrada$Sensibilidad, decreasing = TRUE)
tabla_resumen_DESequilibrada_ordenada <- tabla_resumen_desequilibrada[orden, ]

kable(tabla_resumen_DESequilibrada_ordenada, digits = 3, caption = "Rendimiento de los modelos")
```


## 4.3. Clases equilibradas con RETICULOCITOS

### 4.3.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                    method = "knn",
                   metric = "Kappa",
                    trControl= fitControl)

modelo_knn

# Predicciones
p <- predict(modelo_knn, newdata = dataset.test_retis[,-(ncol(dataset.test_retis))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn

```

### 4.3.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                   method = "nb",
                   metric = "Kappa",
                    trControl= fitControl))
  
# Predicciones
p <- predict(modelo_bayes, dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes
```

### 4.3.4 ANN
```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                    method = "nnet",
                    metric = "Kappa",
                    trControl= fitControl,
                    trace = FALSE))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_ann = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann

```

### 4.3.5 Support Vector Machine

#### 4.3.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                          method = "svmLinear",
                          metric = "Kappa",
                    trControl= fitControl
                          ))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal
```
 
#### 4.2.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                          method = "svmRadial",
                        metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_SVM_radial <- data.frame(
  Modelo              = "SVM-Radial",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial$byClass[1,5]), 0, mconfusion_svmradial$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,5]), 0, mconfusion_svmradial$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,5]), 0, mconfusion_svmradial$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial$byClass[1,1]), 0, mconfusion_svmradial$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,1]), 0, mconfusion_svmradial$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,1]), 0, mconfusion_svmradial$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial$byClass[1,2]), 0, mconfusion_svmradial$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,2]), 0, mconfusion_svmradial$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,2]), 0, mconfusion_svmradial$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svmradial[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial$byClass[1,7]), 0, mconfusion_svmradial$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,7]), 0, mconfusion_svmradial$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,7]), 0, mconfusion_svmradial$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_radial
```

#### 4.3.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                          method = "svmPoly",
                          metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly

```




### 4.3.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                     method = "C5.0",
                     metric = "Kappa",
                    trControl= fitControl
                     ))

# Predicciones
p <- predict(modelo_C5.0, dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol
```



### 4.3.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                   method = "ranger",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test_retis)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF
```

### 4.3.8 Boosting

```{r message=FALSE}
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                   method = "gbm",
                   metric = "Kappa",
                    trControl= fitControl
                   )
```


```{r}
modelo_boosting
# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test_retis)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_boosting <- data.frame(
  Modelo              = "Boosting",
  Precisión           = round((ifelse(is.na(mconfusion_boosting$byClass[1,5]), 0, mconfusion_boosting$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,5]), 0, mconfusion_boosting$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,5]), 0, mconfusion_boosting$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting$byClass[1,1]), 0, mconfusion_boosting$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,1]), 0, mconfusion_boosting$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,1]), 0, mconfusion_boosting$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting$byClass[1,2]), 0, mconfusion_boosting$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,2]), 0, mconfusion_boosting$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,2]), 0, mconfusion_boosting$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_boosting[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting$byClass[1,7]), 0, mconfusion_boosting$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,7]), 0, mconfusion_boosting$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,7]), 0, mconfusion_boosting$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting[["overall"]][["Accuracy"]], 3)
)

resultados_boosting

```


### 4.3.9 Bagging

```{r}
# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train_SMOTE_retis,
                   method = "parRF",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test_retis)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging


```


### 4.3.10 Tabla resumen del rendimiento de los distintos modelos Equilibrados con RETICULOCITOS

```{r}

tabla_resumen_equilibrada_retis = rbind(resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Accuracy
orden = order(tabla_resumen_equilibrada_retis$Sensibilidad, decreasing = TRUE)
tabla_resumen_equilibrada_ordenada <- tabla_resumen_equilibrada_retis[orden, ]

kable(tabla_resumen_equilibrada_ordenada, digits = 3, caption = "Rendimiento de los modelos")
```


## 4.4. Clases DESequilibradas con RETICULOCITOS

### 4.4.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train_retis,
                    method = "knn",
                   metric = "Kappa",
                    trControl= fitControl)

modelo_knn

# Predicciones
p <- predict(modelo_knn, newdata = dataset.test_retis[,-(ncol(dataset.test_retis))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn

```

### 4.4.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train_retis,
                   method = "nb",
                   metric = "Kappa",
                    trControl= fitControl))
  
# Predicciones
p <- predict(modelo_bayes, dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes
```

### 4.4.4 ANN
```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train_retis,
                    method = "nnet",
                    metric = "Kappa",
                    trControl= fitControl,
                    trace = FALSE))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_ann = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann

```

### 4.4.5 Support Vector Machine

#### 4.4.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train_retis,
                          method = "svmLinear",
                          metric = "Kappa",
                    trControl= fitControl
                          ))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal
```
 
#### 4.4.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train_retis,
                          method = "svmRadial",
                        metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_SVM_radial <- data.frame(
  Modelo              = "SVM-Radial",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial$byClass[1,5]), 0, mconfusion_svmradial$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,5]), 0, mconfusion_svmradial$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,5]), 0, mconfusion_svmradial$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial$byClass[1,1]), 0, mconfusion_svmradial$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,1]), 0, mconfusion_svmradial$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,1]), 0, mconfusion_svmradial$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial$byClass[1,2]), 0, mconfusion_svmradial$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,2]), 0, mconfusion_svmradial$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,2]), 0, mconfusion_svmradial$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svmradial[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial$byClass[1,7]), 0, mconfusion_svmradial$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,7]), 0, mconfusion_svmradial$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,7]), 0, mconfusion_svmradial$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_radial
```

#### 4.4.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train_retis,
                          method = "svmPoly",
                          metric = "Kappa",
                    trControl= fitControl))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly

```


### 4.4.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train_retis,
                     method = "C5.0",
                     metric = "Kappa",
                    trControl= fitControl
                     ))

# Predicciones
p <- predict(modelo_C5.0, dataset.test_retis[1:(ncol(dataset.test_retis)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol
```



### 4.4.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train_retis,
                   method = "ranger",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test_retis)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF
```

### 4.4.8 Boosting

```{r message=FALSE}
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train_retis,
                   method = "gbm",
                   metric = "Kappa",
                    trControl= fitControl
                   )
```


```{r}
modelo_boosting
# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test_retis)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_boosting <- data.frame(
  Modelo              = "Boosting",
  Precisión           = round((ifelse(is.na(mconfusion_boosting$byClass[1,5]), 0, mconfusion_boosting$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,5]), 0, mconfusion_boosting$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,5]), 0, mconfusion_boosting$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting$byClass[1,1]), 0, mconfusion_boosting$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,1]), 0, mconfusion_boosting$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,1]), 0, mconfusion_boosting$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting$byClass[1,2]), 0, mconfusion_boosting$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,2]), 0, mconfusion_boosting$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,2]), 0, mconfusion_boosting$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_boosting[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting$byClass[1,7]), 0, mconfusion_boosting$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,7]), 0, mconfusion_boosting$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,7]), 0, mconfusion_boosting$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting[["overall"]][["Accuracy"]], 3)
)

resultados_boosting

```


### 4.4.9 Bagging

```{r}
# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train_retis,
                   method = "parRF",
                   metric = "Kappa",
                    trControl= fitControl
                   ))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test_retis)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test_retis$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging


```


### 4.4.10 Tabla resumen del rendimiento de los distintos modelos DESequilibrados con RETICULOCITOS

```{r}

tabla_resumen_desequilibrada = rbind(resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Accuracy
orden = order(tabla_resumen_desequilibrada$Sensibilidad, decreasing = TRUE)
tabla_resumen_DESequilibrada_ordenada <- tabla_resumen_desequilibrada[orden, ]

kable(tabla_resumen_DESequilibrada_ordenada, digits = 3, caption = "Rendimiento de los modelos")
```


## 4.5 Utilizando tuning grids alternativos con los datos Equilibrados

### 4.5.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- data.frame(k = c(1,2,5,10,15))

# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                    method = "knn",
                    tuneGrid = hiperparametros,
                    metric = "Kappa",
                    trControl = fitControl)

modelo_knn

# Gráfica de la evolución de k
ggplot(modelo_knn) +
  labs(title = "Evolución de Kappa del modelo KNN", x = "K") +
  theme_bw () +
  theme(plot.title = element_text(hjust = 0.5))


# Predicciones
p <- predict(modelo_knn, newdata = dataset.test[,-(ncol(dataset.test))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="NF"))
```



```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn
```

### 4.5.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- expand.grid(usekernel = c(TRUE, FALSE),
                               fL = c(0:4),
                               adjust = c(1:5))

# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "nb",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                    trControl = fitControl))
# Gráfica
ggplot(modelo_bayes) +
  labs(title = "Evolución de Kappa del modelo Naive Bayes", x = "K") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
  
# Predicciones
p <- predict(modelo_bayes, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="NF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes
```

### 4.5.4 ANN
```{r warning=FALSE, message=FALSE}
#hiperparametros 
hiperparametros = expand.grid(size = c(1, 3, 5, 50, 100, 115, 125, 150, 170),
                               decay = c(0.0001, 0.00001, 0.001, 0.01, 0.1))

# Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                    method = "nnet",
                    tuneGrid = hiperparametros,
                    metric = "Kappa",
                    trControl = fitControl,
                    MaxNWts = 2000,
                    trace = FALSE))
# Gráfica
ggplot(modelo_ann, highlight = TRUE) +
  labs(title = "Evolución de Kappa del modelo ANN") +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
res = table(p, dataset.test$Clasificación)
(mconfusion_ann = confusionMatrix(res, mode="everything", positive="NF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann
```

### 4.5.5 Support Vector Machine

#### 4.5.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Hiperparámetros
hiperparametros = data.frame(C = c(0.5,1,5,10,15,20,30, 50))

# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmLinear",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                          trControl = fitControl
                          ))
# Gráfico
ggplot(modelo_svmlineal) +
  labs(title = "Evolución de Kappa del modelo SVMlineal") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="NF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal
```
 
#### 4.5.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
#Hiperparámetros
hiperparametros = expand.grid(sigma = c(0.1,0.5, 0.75, 1, 1.5, 2,3),
                               C = c(1,5,10,15,20,30))

# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmRadial",
                          tuneGrid = hiperparametros,
                         metric = "Kappa",
                    trControl = fitControl))
# Gráfico
ggplot(modelo_svmradial) +
  labs(title = "Evolución de Kappa del modelo SVMRadial") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="NF"))

```

```{r}
resultados_SVM_radial <- data.frame(
  Modelo              = "SVM-Radial",
  Precisión           = round((ifelse(is.na(mconfusion_svmradial$byClass[1,5]), 0, mconfusion_svmradial$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,5]), 0, mconfusion_svmradial$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,5]), 0, mconfusion_svmradial$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmradial$byClass[1,1]), 0, mconfusion_svmradial$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,1]), 0, mconfusion_svmradial$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,1]), 0, mconfusion_svmradial$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmradial$byClass[1,2]), 0, mconfusion_svmradial$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,2]), 0, mconfusion_svmradial$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,2]), 0, mconfusion_svmradial$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svmradial[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmradial$byClass[1,7]), 0, mconfusion_svmradial$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[2,7]), 0, mconfusion_svmradial$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmradial$byClass[3,7]), 0, mconfusion_svmradial$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmradial[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmradial[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_radial
```

#### 4.5.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Hiperparámetros
hiperparametros = expand.grid(degree = c(1, 2,3, 4),
                               scale = c(0.1,0.2,0.3, 0.4, 0.5,0.6,0.7,1),
                               C = c(5,10,15,20,30,40,50,70,80, 90))

# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train_SMOTE,
                          method = "svmPoly",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                    trControl = fitControl))
# Gráfico
ggplot(modelo_svmpoly) +
  labs(title = "Evolución de Kappa del modelo SVMpolynomial") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="NF"))

```

```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly

```




### 4.5.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
#Hiperparámetros
hiperparametros <- expand.grid(trials = c(10:70,80,90, 100),
                               model = c("tree", "rules"),
                               winnow = c(FALSE))

# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                     method = "C5.0",
                     tuneGrid = hiperparametros,
                     metric = "Kappa",
                     trControl = fitControl
                     ))
# Gráfico
ggplot(modelo_C5.0, highlight = TRUE) +
  labs(title = "Evolución de Kappa del modelo C5.0") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_C5.0, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="NF"))

```

```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol
```



### 4.5.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- expand.grid(mtry = c(1:5),
                               min.node.size = c(# 0.5, 
                                                  1,2,3,4),
                               splitrule = c("gini","extratrees"))

# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "ranger",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                   trControl = fitControl,
                   # Número de árboles ajustados
                   num.trees = 500)
)

# Gráfico
ggplot(modelo_forest) +
  labs(title = "Evolución de Kappa del modelo Random Forest") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="NF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF
```

### 4.5.8 Boosting

```{r message=FALSE}
hiperparametros= expand.grid(interaction.depth = c( 3, 5, 10, 20,35, 30), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = c(10,20))
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "gbm",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                   trControl = fitControl
                   )
```


```{r}
modelo_boosting
# Gráfico
ggplot(modelo_boosting) +
  labs(title = "Evolución de Kappa del modelo Boosting") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="NF"))
```

```{r}
resultados_boosting <- data.frame(
  Modelo              = "Boosting",
  Precisión           = round((ifelse(is.na(mconfusion_boosting$byClass[1,5]), 0, mconfusion_boosting$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,5]), 0, mconfusion_boosting$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,5]), 0, mconfusion_boosting$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_boosting$byClass[1,1]), 0, mconfusion_boosting$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,1]), 0, mconfusion_boosting$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,1]), 0, mconfusion_boosting$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_boosting$byClass[1,2]), 0, mconfusion_boosting$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,2]), 0, mconfusion_boosting$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,2]), 0, mconfusion_boosting$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_boosting[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_boosting$byClass[1,7]), 0, mconfusion_boosting$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[2,7]), 0, mconfusion_boosting$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_boosting$byClass[3,7]), 0, mconfusion_boosting$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_boosting[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_boosting[["overall"]][["Accuracy"]], 3)
)

resultados_boosting
```


### 4.5.9 Bagging

```{r}
# Hiperparámetros
hiperparametros= expand.grid(mtry = c(1, 4, 5, 6, 9))

# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train_SMOTE,
                   method = "parRF",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                    trControl = fitControl
                   ))
# Gráfico
ggplot(modelo_Bagging) +
  labs(title = "Evolución de Kappa del modelo Bagging") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="NF"))
```

```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging

```


### 4.5.10 Tabla resumen del rendimiento de los distintos modelos equilibrados con Tuning grids

```{r}

tabla_resumen_equilibrado_10fcv = rbind(resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Sensibilidad
orden = order(tabla_resumen_equilibrado_10fcv$Sensibilidad, decreasing = TRUE)
tabla_resumen_equilibrado_ordenada_10fcv <- tabla_resumen_equilibrado_10fcv[orden, ]

kable(tabla_resumen_equilibrado_ordenada_10fcv, digits = 3, caption = "Rendimiento de los modelos")
```


## 4.6 Utilizando tuning grids alternativos con los datos Desequilibrados
### 4.6.1 k-Nearest Neighbour 

```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- data.frame(k = c(1,5,9,10,15:20))


# modelo
set.seed(29)
modelo_knn <- train(Clasificación ~ ., data = dataset.train,
                    method = "knn",
                    tuneGrid = hiperparametros,
                    metric = "Kappa",
                    trControl = fitControl)

modelo_knn

# Gráfica de la evolución de k
ggplot(modelo_knn) +
  labs(title = "Evolución Kappa del modelo KNN", x = "K") +
  theme_bw () +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_knn, newdata = dataset.test[,-(ncol(dataset.test))])

# Evaluación
(mconfusion_knn <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))
```



```{r}
resultados_knn <- data.frame(
  Modelo        = "KNN",
  Precisión     = round((ifelse(is.na(mconfusion_knn$byClass[1,5]), 0, mconfusion_knn$byClass[1,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,5]), 0, mconfusion_knn$byClass[2,5]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,5]), 0, mconfusion_knn$byClass[3,5])) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_knn$byClass[1,1]), 0, mconfusion_knn$byClass[1,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,1]), 0, mconfusion_knn$byClass[2,1]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,1]), 0, mconfusion_knn$byClass[3,1])) / 3, 3),
  Especificidad = round((ifelse(is.na(mconfusion_knn$byClass[1,2]), 0, mconfusion_knn$byClass[1,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,2]), 0, mconfusion_knn$byClass[2,2]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,2]), 0, mconfusion_knn$byClass[3,2])) / 3, 3),
  kappa         = round(mconfusion_knn[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_knn$byClass[1,7]), 0, mconfusion_knn$byClass[1,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[2,7]), 0, mconfusion_knn$byClass[2,7]) + 
                         ifelse(is.na(mconfusion_knn$byClass[3,7]), 0, mconfusion_knn$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_knn[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_knn[["overall"]][["Accuracy"]], 3)
)

resultados_knn

```

### 4.6.2 Naive Bayes


```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- expand.grid(usekernel = c(TRUE, FALSE),
                               fL = c(0:4),
                               adjust = c(1:4))

# Ajuste del modelo
set.seed(29)
(modelo_bayes = train(Clasificación ~ ., data = dataset.train,
                   method = "nb",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                    trControl = fitControl))
# Gráfica
ggplot(modelo_bayes) +
  labs(title = "Evolución Kappa del modelo Naive Bayes") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_bayes, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_Bayes <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_NaiveBayes <- data.frame(
  Modelo        = "Naive Bayes",
  Precisión     = round((ifelse(is.na(mconfusion_Bayes$byClass[1,5]), 0, mconfusion_Bayes$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,5]), 0, mconfusion_Bayes$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,5]), 0, mconfusion_Bayes$byClass[3,5]) 
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_Bayes$byClass[1,1]), 0, mconfusion_Bayes$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,1]), 0, mconfusion_Bayes$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,1]), 0, mconfusion_Bayes$byClass[3,1])  
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_Bayes$byClass[1,2]), 0, mconfusion_Bayes$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,2]), 0, mconfusion_Bayes$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,2]), 0, mconfusion_Bayes$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_Bayes[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_Bayes$byClass[1,7]), 0, mconfusion_Bayes$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[2,7]), 0, mconfusion_Bayes$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_Bayes$byClass[3,7]), 0, mconfusion_Bayes$byClass[3,7])  
                        ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bayes[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_Bayes[["overall"]][["Accuracy"]], 3)
)

resultados_NaiveBayes
```

### 4.6.4 ANN
```{r warning=FALSE, message=FALSE}
#hiperparametros 
hiperparametros = expand.grid(size = c(3, 10, 20, 50, 100,150),
                               decay = c(0.0001, 0.00001, 0.01,0.1,1))

#Ajuste del modelo 
set.seed(29)

(modelo_ann <- train(Clasificación ~ ., data = dataset.train,
                    method = "nnet",
                    tuneGrid = hiperparametros,
                    metric = "Kappa",
                    trControl = fitControl,
                    MaxNWts = 2000,
                    trace = FALSE))

# Gráfica
ggplot(modelo_ann, highlight = TRUE) +
  labs(title = "Evolución Kappa del modelo ANN") +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))


# Predicciones
p <- predict(modelo_ann, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
res = table(p, dataset.test$Clasificación)
(mconfusion_ann = confusionMatrix(res, mode="everything", positive="AF"))
```

```{r}
resultados_ann <- data.frame(
  Modelo        = "ANN",
  Precisión     = round((ifelse(is.na(mconfusion_ann$byClass[1,5]), 0, mconfusion_ann$byClass[1,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,5]), 0, mconfusion_ann$byClass[2,5]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,5]), 0, mconfusion_ann$byClass[3,5])  
                        ) / 3, 3), 
  Sensibilidad  = round((ifelse(is.na(mconfusion_ann$byClass[1,1]), 0, mconfusion_ann$byClass[1,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,1]), 0, mconfusion_ann$byClass[2,1]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,1]), 0, mconfusion_ann$byClass[3,1]) 
                        ) / 3, 3), 
  Especificidad = round((ifelse(is.na(mconfusion_ann$byClass[1,2]), 0, mconfusion_ann$byClass[1,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,2]), 0, mconfusion_ann$byClass[2,2]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,2]), 0, mconfusion_ann$byClass[3,2])  
                        ) / 3, 3),
  kappa         = round(mconfusion_ann[["overall"]][["Kappa"]], 3),
  F1            = round((ifelse(is.na(mconfusion_ann$byClass[1,7]), 0, mconfusion_ann$byClass[1,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[2,7]), 0, mconfusion_ann$byClass[2,7]) + 
                        ifelse(is.na(mconfusion_ann$byClass[3,7]), 0, mconfusion_ann$byClass[3,7])  
                       ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_ann[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy      = round(mconfusion_ann[["overall"]][["Accuracy"]], 3)
)

resultados_ann
```

### 4.6.5 Support Vector Machine

#### 4.6.5.1 SVM-lineal

```{r warning=FALSE, message=FALSE }
# Hiperparámetros
hiperparametros = data.frame(C = c(0.5,1,5,10,20,30,40,50,75,100))

# Ajuste del modelo 
set.seed(29)
(modelo_svmlineal = train(Clasificación ~ ., data = dataset.train,
                          method = "svmLinear",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                          trControl = fitControl
                          ))
# Gráfico
ggplot(modelo_svmlineal) +
  labs(title = "Evolución Kappa del modelo SVMlineal") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmlineal, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmlineal = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```


```{r}
resultados_svmlineal <- data.frame(
  Modelo              = "SVM lineal",
  Precisión           = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,5]), 0, mconfusion_svmlineal$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,5]), 0, mconfusion_svmlineal$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,5]), 0, mconfusion_svmlineal$byClass[3,5])) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,1]), 0, mconfusion_svmlineal$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,1]), 0, mconfusion_svmlineal$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,1]), 0, mconfusion_svmlineal$byClass[3,1])) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,2]), 0, mconfusion_svmlineal$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,2]), 0, mconfusion_svmlineal$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,2]), 0, mconfusion_svmlineal$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_svmlineal[["overall"]][["Kappa"]], 3),
  
  F1                  = round((ifelse(is.na(mconfusion_svmlineal$byClass[1,7]), 0, mconfusion_svmlineal$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[2,7]), 0, mconfusion_svmlineal$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmlineal$byClass[3,7]), 0, mconfusion_svmlineal$byClass[3,7])) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmlineal[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmlineal[["overall"]][["Accuracy"]], 3)
)

resultados_svmlineal
```
 
#### 4.6.5.2 SVM-RBF o función gaussiana

```{r warning=FALSE, message=FALSE}
#Hiperparámetros
hiperparametros = expand.grid(sigma = c(0.05, 0.1, 0.15,0.1958621, 0.2,0.5), 
                               C = c(1:3,5,10,15,20))

# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train,
                          method = "svmRadial",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                          trControl = fitControl))
# Gráfico
ggplot(modelo_svmradial) +
  labs(title = "Evolución Kappa del modelo SVMRadial") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
#Hiperparámetros
hiperparametros = expand.grid(sigma = c(2^seq(-15,3,2)), 
                               C = c(2^seq(-5,15,2)))

# Ajuste del modelo 
set.seed(29)
(modelo_svmradial = train(Clasificación ~ ., data = dataset.train,
                          method = "svmRadial",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                          trControl = fitControl))
# Gráfico
ggplot(modelo_svmradial) +
  labs(title = "Evolución Kappa del modelo SVMRadial") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmradial, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmradial = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```



#### 4.6.5.3 SVM-polynomial
```{r warning=FALSE, message=FALSE} 
# Hiperparámetros
hiperparametros = expand.grid(degree = c(2,3),
                               scale = c(0.1,0.15,0.2,0.25,0.3,0.5),
                               C = c(1,5,10,15,20,30))

# Ajuste del modelo 
set.seed(29)
(modelo_svmpoly = train(Clasificación ~ ., data = dataset.train,
                          method = "svmPoly",
                          tuneGrid = hiperparametros,
                          metric = "Kappa",
                          trControl = fitControl))
# Gráfico
ggplot(modelo_svmpoly) +
  labs(title = "Evolución Kappa del modelo SVMpolynomial") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p = predict(modelo_svmpoly, newdata = dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_svmpoly = confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_SVM_poly <- data.frame(
  Modelo              = "SVM-poly",
  Precisión           = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,5]), 0, mconfusion_svmpoly$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,5]), 0, mconfusion_svmpoly$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,5]), 0, mconfusion_svmpoly$byClass[3,5]) ) / 3, 3),
  
  Sensibilidad        = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,1]), 0, mconfusion_svmpoly$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,1]), 0, mconfusion_svmpoly$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,1]), 0, mconfusion_svmpoly$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,2]), 0, mconfusion_svmpoly$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,2]), 0, mconfusion_svmpoly$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,2]), 0, mconfusion_svmpoly$byClass[3,2]) ) /3, 3),
  kappa               = round(mconfusion_svmpoly[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svmpoly$byClass[1,7]), 0, mconfusion_svmpoly$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[2,7]), 0, mconfusion_svmpoly$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svmpoly$byClass[3,7]), 0, mconfusion_svmpoly$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svmpoly[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svmpoly[["overall"]][["Accuracy"]], 3)
)

resultados_SVM_poly

```


### 4.6.6 Árbol de decisión

```{r warning=FALSE, message=FALSE}
#Hiperparámetros
hiperparametros <- expand.grid(trials = c(10:40),
                               model = c("tree","rules"),
                               winnow = c(FALSE,TRUE))

# Ajuste del modelo
set.seed(29)
(modelo_C5.0 <- train(Clasificación ~ ., data = dataset.train,
                     method = "C5.0",
                     tuneGrid = hiperparametros,
                     metric = "Kappa",
                     trControl = fitControl
                     ))
# Gráfico
ggplot(modelo_C5.0, highlight = TRUE) +
  labs(title = "Evolución Kappa del modelo C5.0") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_C5.0, dataset.test[1:(ncol(dataset.test)-1)])

# Evaluación
(mconfusion_arbol = confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))

```

```{r}
resultados_arbol <- data.frame(
  Modelo              = "Árbol",
  Precisión           = round((ifelse(is.na(mconfusion_arbol$byClass[1,5]), 0, mconfusion_arbol$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,5]), 0, mconfusion_arbol$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,5]), 0, mconfusion_arbol$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_arbol$byClass[1,1]), 0, mconfusion_arbol$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,1]), 0, mconfusion_arbol$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,1]), 0, mconfusion_arbol$byClass[3,1]) ) / 3, 3),
  
  Especificidad       = round((ifelse(is.na(mconfusion_arbol$byClass[1,2]), 0, mconfusion_arbol$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,2]), 0, mconfusion_arbol$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,2]), 0, mconfusion_arbol$byClass[3,2])) / 3, 3),
  
  kappa               = round(mconfusion_arbol[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_arbol$byClass[1,7]), 0, mconfusion_arbol$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[2,7]), 0, mconfusion_arbol$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_arbol$byClass[3,7]), 0, mconfusion_arbol$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_arbol[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_arbol[["overall"]][["Accuracy"]], 3)
)

resultados_arbol
```



### 4.6.7 Random Forest

```{r warning=FALSE, message=FALSE}
# Hiperparámetros
hiperparametros <- expand.grid(mtry = c(1:5),
                               min.node.size = c(1,2,3,4),
                               splitrule = c("gini","extratrees"))

# Ajuste del modelo
set.seed(29)
(modelo_forest <- train(Clasificación ~ ., data = dataset.train,
                   method = "ranger",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                   trControl = fitControl,
                   # Número de árboles ajustados
                   num.trees = 500
                   ))
# Gráfico
ggplot(modelo_forest) +
  labs(title = "Evolución Kappa del modelo Random Forest") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_forest, newdata = dataset.test)


# Evaluación
(mconfusion_forest <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```


```{r}
resultados_RF <- data.frame(
  Modelo              = "Random Forest",
  Precisión           = round((ifelse(is.na(mconfusion_forest$byClass[1,5]), 0, mconfusion_forest$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,5]), 0, mconfusion_forest$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,5]), 0, mconfusion_forest$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_forest$byClass[1,1]), 0, mconfusion_forest$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,1]), 0, mconfusion_forest$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,1]), 0, mconfusion_forest$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_forest$byClass[1,2]), 0, mconfusion_forest$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,2]), 0, mconfusion_forest$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,2]), 0, mconfusion_forest$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_forest[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_forest$byClass[1,7]), 0, mconfusion_forest$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[2,7]), 0, mconfusion_forest$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_forest$byClass[3,7]), 0, mconfusion_forest$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_forest[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_forest[["overall"]][["Accuracy"]], 3)
)

resultados_RF

```

### 4.6.8 Boosting

```{r message=FALSE}
hiperparametros= expand.grid(interaction.depth = c(1, 5:9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
# Ajuste del modelo
set.seed(29)
modelo_boosting <- train(Clasificación ~ ., data = dataset.train,
                   method = "gbm",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                    trControl = fitControl
                   )
```


```{r}
modelo_boosting
# Gráfico
ggplot(modelo_boosting) +
  labs(title = "Evolución Kappa del modelo Boosting") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_boosting, newdata = dataset.test)


# Evaluación
(mconfusion_boosting <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```



### 4.6.9 Bagging

```{r}
# Hiperparámetros
hiperparametros= expand.grid(mtry = c(1:5, 9))

# Ajuste del modelo
set.seed(29)
(modelo_Bagging <- train(Clasificación ~ ., data = dataset.train,
                   method = "parRF",
                   tuneGrid = hiperparametros,
                   metric = "Kappa",
                    trControl = fitControl
                   ))
                   
# Gráfico
ggplot(modelo_Bagging) +
  labs(title = "Evolución del Kappa del modelo Bagging") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Predicciones
p <- predict(modelo_Bagging, newdata = dataset.test)


# Evaluación
(mconfusion_Bagging <- confusionMatrix(data = p, reference = dataset.test$Clasificación, mode="everything", positive="AF"))
```

```{r}
resultados_bagging <- data.frame(
  Modelo              = "Bagging",
  Precisión           = round((ifelse(is.na(mconfusion_Bagging$byClass[1,5]), 0, mconfusion_Bagging$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,5]), 0, mconfusion_Bagging$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,5]), 0, mconfusion_Bagging$byClass[3,5])  ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_Bagging$byClass[1,1]), 0, mconfusion_Bagging$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,1]), 0, mconfusion_Bagging$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,1]), 0, mconfusion_Bagging$byClass[3,1])  ) /3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_Bagging$byClass[1,2]), 0, mconfusion_Bagging$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,2]), 0, mconfusion_Bagging$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,2]), 0, mconfusion_Bagging$byClass[3,2])  ) / 3, 3),
  kappa               = round(mconfusion_Bagging[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_Bagging$byClass[1,7]), 0, mconfusion_Bagging$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[2,7]), 0, mconfusion_Bagging$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_Bagging$byClass[3,7]), 0, mconfusion_Bagging$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_Bagging[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_Bagging[["overall"]][["Accuracy"]], 3)
)

resultados_bagging


```
`


### 4.6.10 Tabla resumen del rendimiento de los distintos modelos DESequilibrados con Tuning grids

```{r}

tabla_resumen_desequilibrada_tuning = rbind(resultados_knn,
                     resultados_NaiveBayes,
                     resultados_ann,
                     resultados_SVM_radial,
                     resultados_svmlineal,
                     resultados_SVM_poly,
                     resultados_arbol, 
                     resultados_RF,
                     resultados_boosting,
                     resultados_bagging
                     )

#Establecemos el orden por valores de Sensibilidad
orden = order(tabla_resumen_desequilibrada_tuning$Sensibilidad, decreasing = TRUE)
tabla_resumen_desequilibrada_tuning_ordenada <- tabla_resumen_desequilibrada_tuning[orden, ]

kable(tabla_resumen_desequilibrada_tuning_ordenada, digits = 3, caption = "Rendimiento de los modelos desequilibrados con Tuning grids personalizados")
```

## 4.7 Aplicar pesos a las clases

```{r}
table(dataset.train$Clasificación) 
total_samples= nrow(dataset.train)
class_weights <- c("AF" = total_samples/ nrow(filter(dataset.train, dataset.train$Clasificación== "AF")), 
                   "FF/FL" = total_samples/ nrow(filter(dataset.train, dataset.train$Clasificación== "FF/FL")), 
                   "NF" = total_samples/ nrow(filter(dataset.train, dataset.train$Clasificación== "NF")))
class_weights
```

### 4.7.1 Datos equilibrados 

#### 4.7.1.1 Paquete RandomForest 

Haremos el fine tuning manual: 
```{r}
library(randomForest)

set.seed(29)
# función de validación cruzada
cross_validation <- function(data, k_folds, mtry_values, ntree_values, nodesize_values, class_weights_values, seed=29) {
  set.seed(seed)
  folds <- createFolds(data$Clasificación, k = k_folds, list = TRUE)
  results <- data.frame()
  
  for (mtry in mtry_values) {
    for (ntree in ntree_values) {
      for (nodesize in nodesize_values) {
        for (classwt in class_weights_values) {
          fold_results <- c()
          
          for (fold in folds) {
            train_data <- data[-fold,]
            test_data <- data[fold,]
            
            model <- randomForest(Clasificación ~ ., data = train_data, 
                                  mtry = mtry, ntree = ntree, nodesize = nodesize, classwt = classwt)
            predictions <- predict(model, newdata = test_data)
            cm <- confusionMatrix(predictions, test_data$Clasificación)
            
            fold_results <- c(fold_results, cm$overall['Kappa'])
          }
          
          mean_kappa <- mean(fold_results)
          results <- rbind(results, data.frame(mtry = mtry, ntree = ntree, nodesize = nodesize, classwt = paste(classwt, collapse=","), Kappa = mean_kappa))
        }
      }
    }
  }
  
  return(results)
}

# hiperparámetros 
mtry_values <- 1:(ncol(dataset.train_SMOTE) - 1)
ntree_values <- c(100, 150, 200,300)
nodesize_values <- c(1, 2, 3, 4, 5)
class_weights_values <- list(c(5,4,1), c(4,4,1),c(3,4,1), c(3,3,1), c(2,2,1))
    
# validación cruzada
results <- cross_validation(dataset.train_SMOTE, k_folds = 10, mtry_values = mtry_values, ntree_values = ntree_values, nodesize_values = nodesize_values, class_weights_values = class_weights_values)

# Mostrar los resultados
print(results)

# Encontrar la mejor combinación de hiperparámetros
best_params <- results[which.max(results$Kappa),]
best_params


```

```{r}
# Convertir classwt de cadena a lista
best_classwt <- as.numeric(unlist(strsplit(best_params$classwt, ",")))

# Entrenar el modelo final con los mejores hiperparámetros
set.seed(29)
modelo_forest_rf <- randomForest(
  Clasificación ~ ., 
  data = dataset.train_SMOTE, 
  mtry = as.numeric(best_params$mtry), 
  ntree = as.numeric(best_params$ntree), 
  nodesize = as.numeric(best_params$nodesize), 
  classwt = best_classwt
)

# Predicciones
p <- predict(modelo_forest_rf, newdata = dataset.test)

# Evaluación
mconfusion_rf_rf_pesos <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF")
print(mconfusion_rf_rf_pesos)



```


```{r}
resultados_RF_rf <- data.frame(
  Modelo              = "Random Forest con pesos de RF",
  Precisión           = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,5]), 0, mconfusion_rf_rf_pesos$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,5]), 0, mconfusion_rf_rf_pesos$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,5]), 0, mconfusion_rf_rf_pesos$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,1]), 0, mconfusion_rf_rf_pesos$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,1]), 0, mconfusion_rf_rf_pesos$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,1]), 0, mconfusion_rf_rf_pesos$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,2]), 0, mconfusion_rf_rf_pesos$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,2]), 0, mconfusion_rf_rf_pesos$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,2]), 0, mconfusion_rf_rf_pesos$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_rf_rf_pesos[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,7]), 0, mconfusion_rf_rf_pesos$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,7]), 0, mconfusion_rf_rf_pesos$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,7]), 0, mconfusion_rf_rf_pesos$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_rf_rf_pesos[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_rf_rf_pesos[["overall"]][["Accuracy"]], 3)
)

resultados_RF_rf

```


#### 4.7.1.2 SVM del paquete e1071

##### 4.7.1.2.1 SVM lineal

```{r}
library(e1071)
set.seed(29)

# Definir hiperparámetros
cost_values <- c(0.001, 0.01, 0.1, 1, 5, 10, 50)
class_weights_list <- list(
  c(AF = 5, `FF/FL` = 4, NF = 1),
  c(AF = 4, `FF/FL` = 4, NF = 1),
  c(AF = 3, `FF/FL` = 4, NF = 1),
  c(AF = 3, `FF/FL` = 3, NF = 1)
)

# Inicializar variables para almacenar el mejor modelo y parámetros
best_kappa <- -Inf
best_model <- NULL
best_params <- list()

# Realizar ajuste fino con validación cruzada de 10-fold
for (cost in cost_values) {
  for (class_weights in class_weights_list) {
    # Inicializar vector para almacenar los Kappa obtenidos en cada fold
    kappa_values <- numeric(10)
    
    # Realizar validación cruzada de 10-fold
    folds <- createFolds(dataset.train$Clasificación, k = 10)
    for (i in 1:10) {
      train_indices <- unlist(folds[-i])
      test_indices <- folds[[i]]
      
      # Entrenar el modelo SVM con los parámetros actuales
      current_model <- svm(
        Clasificación ~ ., 
        data = dataset.train_SMOTE[train_indices, ], 
        kernel = "linear",
        cost = cost, 
        class.weights = class_weights
      )
      
      # Realizar predicciones en el conjunto de prueba del fold actual
      predictions <- predict(current_model, newdata = dataset.train_SMOTE[test_indices, ])
      
      # Calcular matriz de confusión para el fold actual y obtener Kappa
      cm <- confusionMatrix(predictions, dataset.train_SMOTE$Clasificación[test_indices], mode = "everything", positive = "AF")
      kappa_values[i] <- cm$overall['Kappa']
    }
    
    # Calcular el Kappa promedio de los 10 folds
    current_kappa <- mean(kappa_values)
    
    # Actualizar el mejor modelo y parámetros si se encuentra un Kappa mejor
    if (current_kappa > best_kappa) {
      best_kappa <- current_kappa
      best_model <- current_model
      best_params <- list(cost = cost, class_weights = class_weights)
    }
  }
}

# Mostrar los mejores resultados encontrados
print(best_params)
print(best_model)

# Realizar predicciones con el mejor modelo encontrado
p <- predict(best_model, newdata = dataset.test)

# Evaluación final del modelo
mconfusion_svm_lineal_e <- confusionMatrix(p, dataset.test$Clasificación, mode = "everything", positive = "AF")
mconfusion_svm_lineal_e

```

```{r}
resultados_svm_lineal_pesos <- data.frame(
  Modelo              = "SVM lineal con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,5]), 0, mconfusion_svm_lineal_e$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,5]), 0, mconfusion_svm_lineal_e$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,5]), 0, mconfusion_svm_lineal_e$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,1]), 0, mconfusion_svm_lineal_e$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,1]), 0, mconfusion_svm_lineal_e$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,1]), 0, mconfusion_svm_lineal_e$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,2]), 0, mconfusion_svm_lineal_e$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,2]), 0, mconfusion_svm_lineal_e$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,2]), 0, mconfusion_svm_lineal_e$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_lineal_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,7]), 0, mconfusion_svm_lineal_e$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,7]), 0, mconfusion_svm_lineal_e$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,7]), 0, mconfusion_svm_lineal_e$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_lineal_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_lineal_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_lineal_pesos

```


##### 4.7.1.2.2 SVM función radial 

```{r}

set.seed(29)

# Definir los hiperparámetros a ajustar
cost_values <- c(0.1, 1, 10, 100, 1000)
gamma_values <- c(0.5, 1, 2, 3, 4)
class_weights_list <- list(
  c("AF" = 5, "FF/FL" = 4, "NF" = 1),
  c("AF" = 4, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 3, "NF" = 1)
)

# Lista para guardar los resultados
tune_results <- list()

# Loop para ajuste fino
for (class_weights in class_weights_list) {
  # Ajuste de los parámetros cost y gamma
  tune_r <- tune.svm(
    Clasificación ~ ., 
    data = dataset.train_SMOTE, 
    kernel = "radial",
    cost = cost_values,
    gamma = gamma_values,
    class.weights = class_weights
  )
  
  # Guardar los resultados de cada combinación de class_weights
  tune_results[[paste(names(class_weights), collapse = "_")]] <- tune_r
}

# Seleccionar el mejor modelo basado en el error
best_tune <- tune_results[[which.min(sapply(tune_results, function(x) x$best.performance))]]
best_model <- best_tune$best.model

# Entrenar el mejor modelo con los mejores parámetros y pesos de clase
svm_r <- svm(
  Clasificación ~ ., 
  data = dataset.train_SMOTE,
  type = "C-classification",
  kernel = "radial",
  cost = best_model$cost,
  gamma = best_model$gamma,
  class.weights = best_model$class.weights,
  probability = TRUE
)


pred_svm <- predict(svm_r, newdata = dataset.test, probability = TRUE)

# Evaluación
(mconfusion_svm_radial_e <- confusionMatrix(pred_svm, dataset.test$Clasificación, mode="everything", positive="AF"))

```



```{r}
resultados_svm_radial_pesos= data.frame(
  Modelo              = "SVM radial con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,5]), 0, mconfusion_svm_radial_e$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,5]), 0, mconfusion_svm_radial_e$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,5]), 0, mconfusion_svm_radial_e$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,1]), 0, mconfusion_svm_radial_e$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,1]), 0, mconfusion_svm_radial_e$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,1]), 0, mconfusion_svm_radial_e$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,2]), 0, mconfusion_svm_radial_e$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,2]), 0, mconfusion_svm_radial_e$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,2]), 0, mconfusion_svm_radial_e$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_radial_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,7]), 0, mconfusion_svm_radial_e$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,7]), 0, mconfusion_svm_radial_e$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,7]), 0, mconfusion_svm_radial_e$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_radial_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_radial_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_radial_pesos
```


##### 4.7.1.2.3 SVM polynomial 
```{r}
library(e1071)
set.seed(29)

# Definir los hiperparámetros a ajustar
cost_values <- c(0.1, 1, 10, 100, 1000)
degree_values <- c(2, 3, 4, 5)
class_weights_list <- list(
  c("AF" = 5, "FF/FL" = 4, "NF" = 1),
  c("AF" = 4, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 3, "NF" = 1)
)

# Lista para guardar los resultados
tune_results <- list()

# Loop para ajuste fino
for (class_weights in class_weights_list) {
  # Ajuste de los parámetros cost y degree
  tune_p <- tune.svm(
    Clasificación ~ ., 
    data = dataset.train_SMOTE, 
    kernel = "polynomial",
    cost = cost_values,
    degree = degree_values,
    class.weights = class_weights
  )
  
  # Guardar los resultados de cada combinación de class_weights
  tune_results[[paste(names(class_weights), collapse = "_")]] <- tune_p
}

# Seleccionar el mejor modelo basado en el error
best_tune <- tune_results[[which.min(sapply(tune_results, function(x) x$best.performance))]]
best_model <- best_tune$best.model

# Entrenar el mejor modelo con los mejores parámetros y pesos de clase
svm_p <- svm(
  Clasificación ~ ., 
  data = dataset.train_SMOTE,
  type = "C-classification",
  kernel = "polynomial",
  cost = best_model$cost,
  degree = best_model$degree,
  class.weights = best_model$class.weights,
  probability = TRUE
)

# Predicción con el mejor modelo
pred_svm <- predict(svm_p, newdata = dataset.test, probability = TRUE)

# Evaluación
(mconfusion_svm_polynomial_e = confusionMatrix(pred_svm, dataset.test$Clasificación, mode = "everything", positive = "AF"))


```



```{r}
resultados_svm_polynomial_pesos = data.frame(
  Modelo              = "SVM polinomial con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 5]), 0, mconfusion_svm_polynomial_e$byClass[1, 5]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 5]), 0, mconfusion_svm_polynomial_e$byClass[2, 5]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 5]), 0, mconfusion_svm_polynomial_e$byClass[3, 5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 1]), 0, mconfusion_svm_polynomial_e$byClass[1, 1]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 1]), 0, mconfusion_svm_polynomial_e$byClass[2, 1]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 1]), 0, mconfusion_svm_polynomial_e$byClass[3, 1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 2]), 0, mconfusion_svm_polynomial_e$byClass[1, 2]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 2]), 0, mconfusion_svm_polynomial_e$byClass[2, 2]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 2]), 0, mconfusion_svm_polynomial_e$byClass[3, 2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_polynomial_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 7]), 0, mconfusion_svm_polynomial_e$byClass[1, 7]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 7]), 0, mconfusion_svm_polynomial_e$byClass[2, 7]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 7]), 0, mconfusion_svm_polynomial_e$byClass[3, 7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_polynomial_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_polynomial_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_polynomial_pesos

```


#### 4.7.1.3 Tabla resumen del rendimiento de los distintos modelos equilibrando pesos con dataset equilibrado con SMOTE
```{r}

tabla_resumen_equilibrado_pesos = rbind(resultados_RF_rf,
                                        #resultados_RF_pesos,
                                        #resultados_xgboost,
                                        resultados_svm_lineal_pesos,
                                        resultados_svm_radial_pesos,
                                        resultados_svm_polynomial_pesos
                                        #resultados_glmnet
                                        )

#Establecemos el orden por valores de Sensibilidad
orden = order(tabla_resumen_equilibrado_pesos$Sensibilidad, decreasing = TRUE)
tabla_resumen_equilibrado_pesos_ordenada <- tabla_resumen_equilibrado_pesos[orden, ]

kable(tabla_resumen_equilibrado_pesos_ordenada, digits = 3, caption = "Rendimiento de los modelos ajustando los pesos")
```


### 4.7.2 Datos desequilibrados

#### 4.7.2.1 Paquete RandomForest 

Haremos el fine tuning manual: 
```{r}
library(randomForest)

set.seed(29)
# función de validación cruzada
cross_validation <- function(data, k_folds, mtry_values, ntree_values, nodesize_values, class_weights_values, seed=29) {
  set.seed(seed)
  folds <- createFolds(data$Clasificación, k = k_folds, list = TRUE)
  results <- data.frame()
  
  for (mtry in mtry_values) {
    for (ntree in ntree_values) {
      for (nodesize in nodesize_values) {
        for (classwt in class_weights_values) {
          fold_results <- c()
          
          for (fold in folds) {
            train_data <- data[-fold,]
            test_data <- data[fold,]
            
            model <- randomForest(Clasificación ~ ., data = train_data, 
                                  mtry = mtry, ntree = ntree, nodesize = nodesize, classwt = classwt)
            predictions <- predict(model, newdata = test_data)
            cm <- confusionMatrix(predictions, test_data$Clasificación)
            
            fold_results <- c(fold_results, cm$overall['Kappa'])
          }
          
          mean_kappa <- mean(fold_results)
          results <- rbind(results, data.frame(mtry = mtry, ntree = ntree, nodesize = nodesize, classwt = paste(classwt, collapse=","), Kappa = mean_kappa))
        }
      }
    }
  }
  
  return(results)
}

# hiperparámetros 
mtry_values <- 1:(ncol(dataset.train) - 1)
ntree_values <- c(100, 150, 200,300)
nodesize_values <- c(1, 2, 3, 4, 5)
class_weights_values <- list(c(5,4,1), c(4,4,1),c(3,4,1), c(3,3,1))
    
# validación cruzada
results <- cross_validation(dataset.train, k_folds = 10, mtry_values = mtry_values, ntree_values = ntree_values, nodesize_values = nodesize_values, class_weights_values = class_weights_values)

# Mostrar los resultados
print(results)

# Encontrar la mejor combinación de hiperparámetros
best_params <- results[which.max(results$Kappa),]
best_params


```
```{r}
# Convertir classwt de cadena a lista
best_classwt <- as.numeric(unlist(strsplit(best_params$classwt, ",")))

# Entrenar el modelo final con los mejores hiperparámetros
set.seed(29)
modelo_forest_rf <- randomForest(
  Clasificación ~ ., 
  data = dataset.train, 
  mtry = as.numeric(best_params$mtry), 
  ntree = as.numeric(best_params$ntree), 
  nodesize = as.numeric(best_params$nodesize), 
  classwt = best_classwt
)

# Predicciones
p <- predict(modelo_forest_rf, newdata = dataset.test)

# Evaluación
mconfusion_rf_rf_pesos <- confusionMatrix(p, dataset.test$Clasificación, mode="everything", positive="AF")
print(mconfusion_rf_rf_pesos)



```


```{r}
resultados_RF_rf <- data.frame(
  Modelo              = "Random Forest con pesos de RF",
  Precisión           = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,5]), 0, mconfusion_rf_rf_pesos$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,5]), 0, mconfusion_rf_rf_pesos$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,5]), 0, mconfusion_rf_rf_pesos$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,1]), 0, mconfusion_rf_rf_pesos$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,1]), 0, mconfusion_rf_rf_pesos$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,1]), 0, mconfusion_rf_rf_pesos$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,2]), 0, mconfusion_rf_rf_pesos$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,2]), 0, mconfusion_rf_rf_pesos$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,2]), 0, mconfusion_rf_rf_pesos$byClass[3,2])) / 3, 3),
  kappa               = round(mconfusion_rf_rf_pesos[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_rf_rf_pesos$byClass[1,7]), 0, mconfusion_rf_rf_pesos$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[2,7]), 0, mconfusion_rf_rf_pesos$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_rf_rf_pesos$byClass[3,7]), 0, mconfusion_rf_rf_pesos$byClass[3,7]) ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_rf_rf_pesos[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_rf_rf_pesos[["overall"]][["Accuracy"]], 3)
)

resultados_RF_rf

```


#### 4.7.2.2 SVM del paquete e1071

##### 4.7.2.2.1 SVM lineal

```{r}
library(e1071)
set.seed(29)

# Definir hiperparámetros
cost_values <- c(0.001, 0.01, 0.1, 1, 5, 10, 50)
class_weights_list <- list(
  c(AF = 5, `FF/FL` = 4, NF = 1),
  c(AF = 4, `FF/FL` = 4, NF = 1),
  c(AF = 3, `FF/FL` = 4, NF = 1),
  c(AF = 3, `FF/FL` = 3, NF = 1)
)

# Inicializar variables para almacenar el mejor modelo y parámetros
best_kappa <- -Inf
best_model <- NULL
best_params <- list()

# Realizar ajuste fino con validación cruzada de 10-fold
for (cost in cost_values) {
  for (class_weights in class_weights_list) {
    # Inicializar vector para almacenar los Kappa obtenidos en cada fold
    kappa_values <- numeric(10)
    
    # Realizar validación cruzada de 10-fold
    folds <- createFolds(dataset.train$Clasificación, k = 10)
    for (i in 1:10) {
      train_indices <- unlist(folds[-i])
      test_indices <- folds[[i]]
      
      # Entrenar el modelo SVM con los parámetros actuales
      current_model <- svm(
        Clasificación ~ ., 
        data = dataset.train[train_indices, ], 
        kernel = "linear",
        cost = cost, 
        class.weights = class_weights
      )
      
      # Realizar predicciones en el conjunto de prueba del fold actual
      predictions <- predict(current_model, newdata = dataset.train[test_indices, ])
      
      # Calcular matriz de confusión para el fold actual y obtener Kappa
      cm <- confusionMatrix(predictions, dataset.train$Clasificación[test_indices], mode = "everything", positive = "AF")
      kappa_values[i] <- cm$overall['Kappa']
    }
    
    # Calcular el Kappa promedio de los 10 folds
    current_kappa <- mean(kappa_values)
    
    # Actualizar el mejor modelo y parámetros si se encuentra un Kappa mejor
    if (current_kappa > best_kappa) {
      best_kappa <- current_kappa
      best_model <- current_model
      best_params <- list(cost = cost, class_weights = class_weights)
    }
  }
}

# Mostrar los mejores resultados encontrados
print(best_params)
print(best_model)

# Realizar predicciones con el mejor modelo encontrado
p <- predict(best_model, newdata = dataset.test)

# Evaluación final del modelo
mconfusion_svm_lineal_e <- confusionMatrix(p, dataset.test$Clasificación, mode = "everything", positive = "AF")
mconfusion_svm_lineal_e

```

```{r}
resultados_svm_lineal_pesos <- data.frame(
  Modelo              = "SVM lineal con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,5]), 0, mconfusion_svm_lineal_e$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,5]), 0, mconfusion_svm_lineal_e$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,5]), 0, mconfusion_svm_lineal_e$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,1]), 0, mconfusion_svm_lineal_e$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,1]), 0, mconfusion_svm_lineal_e$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,1]), 0, mconfusion_svm_lineal_e$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,2]), 0, mconfusion_svm_lineal_e$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,2]), 0, mconfusion_svm_lineal_e$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,2]), 0, mconfusion_svm_lineal_e$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_lineal_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_lineal_e$byClass[1,7]), 0, mconfusion_svm_lineal_e$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[2,7]), 0, mconfusion_svm_lineal_e$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svm_lineal_e$byClass[3,7]), 0, mconfusion_svm_lineal_e$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_lineal_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_lineal_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_lineal_pesos

```


##### 4.7.2.2.2 SVM función radial 

```{r}

set.seed(29)

# Definir los hiperparámetros a ajustar
cost_values <- c(0.1, 1, 10, 100, 1000)
gamma_values <- c(0.5, 1, 2, 3, 4)
class_weights_list <- list(
  c("AF" = 5, "FF/FL" = 4, "NF" = 1),
  c("AF" = 4, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 3, "NF" = 1)
)

# Lista para guardar los resultados
tune_results <- list()

# Loop para ajuste fino
for (class_weights in class_weights_list) {
  # Ajuste de los parámetros cost y gamma
  tune_r <- tune.svm(
    Clasificación ~ ., 
    data = dataset.train, 
    kernel = "radial",
    cost = cost_values,
    gamma = gamma_values,
    class.weights = class_weights
  )
  
  # Guardar los resultados de cada combinación de class_weights
  tune_results[[paste(names(class_weights), collapse = "_")]] <- tune_r
}

# Seleccionar el mejor modelo basado en el error
best_tune <- tune_results[[which.min(sapply(tune_results, function(x) x$best.performance))]]
best_model <- best_tune$best.model

# Entrenar el mejor modelo con los mejores parámetros y pesos de clase
svm_r <- svm(
  Clasificación ~ ., 
  data = dataset.train,
  type = "C-classification",
  kernel = "radial",
  cost = best_model$cost,
  gamma = best_model$gamma,
  class.weights = best_model$class.weights,
  probability = TRUE
)


pred_svm <- predict(svm_r, newdata = dataset.test, probability = TRUE)

# Evaluación
(mconfusion_svm_radial_e <- confusionMatrix(pred_svm, dataset.test$Clasificación, mode="everything", positive="AF"))

```



```{r}
resultados_svm_radial_pesos= data.frame(
  Modelo              = "SVM radial con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,5]), 0, mconfusion_svm_radial_e$byClass[1,5]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,5]), 0, mconfusion_svm_radial_e$byClass[2,5]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,5]), 0, mconfusion_svm_radial_e$byClass[3,5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,1]), 0, mconfusion_svm_radial_e$byClass[1,1]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,1]), 0, mconfusion_svm_radial_e$byClass[2,1]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,1]), 0, mconfusion_svm_radial_e$byClass[3,1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,2]), 0, mconfusion_svm_radial_e$byClass[1,2]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,2]), 0, mconfusion_svm_radial_e$byClass[2,2]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,2]), 0, mconfusion_svm_radial_e$byClass[3,2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_radial_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_radial_e$byClass[1,7]), 0, mconfusion_svm_radial_e$byClass[1,7]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[2,7]), 0, mconfusion_svm_radial_e$byClass[2,7]) + 
                              ifelse(is.na(mconfusion_svm_radial_e$byClass[3,7]), 0, mconfusion_svm_radial_e$byClass[3,7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_radial_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_radial_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_radial_pesos
```


##### 4.7.2.2.3 SVM polynomial 
```{r}
library(e1071)
set.seed(29)

# Definir los hiperparámetros a ajustar
cost_values <- c(0.1, 1, 10, 100, 1000)
degree_values <- c(2, 3, 4, 5)
class_weights_list <- list(
  c("AF" = 5, "FF/FL" = 4, "NF" = 1),
  c("AF" = 4, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 4, "NF" = 1),
  c("AF" = 3, "FF/FL" = 3, "NF" = 1)
)

# Lista para guardar los resultados
tune_results <- list()

# Loop para ajuste fino
for (class_weights in class_weights_list) {
  # Ajuste de los parámetros cost y degree
  tune_p <- tune.svm(
    Clasificación ~ ., 
    data = dataset.train, 
    kernel = "polynomial",
    cost = cost_values,
    degree = degree_values,
    class.weights = class_weights
  )
  
  # Guardar los resultados de cada combinación de class_weights
  tune_results[[paste(names(class_weights), collapse = "_")]] <- tune_p
}

# Seleccionar el mejor modelo basado en el error
best_tune <- tune_results[[which.min(sapply(tune_results, function(x) x$best.performance))]]
best_model <- best_tune$best.model

# Entrenar el mejor modelo con los mejores parámetros y pesos de clase
svm_p <- svm(
  Clasificación ~ ., 
  data = dataset.train,
  type = "C-classification",
  kernel = "polynomial",
  cost = best_model$cost,
  degree = best_model$degree,
  class.weights = best_model$class.weights,
  probability = TRUE
)

# Predicción con el mejor modelo
pred_svm <- predict(svm_p, newdata = dataset.test, probability = TRUE)

# Evaluación
(mconfusion_svm_polynomial_e = confusionMatrix(pred_svm, dataset.test$Clasificación, mode = "everything", positive = "AF"))


```



```{r}
resultados_svm_polynomial_pesos = data.frame(
  Modelo              = "SVM polinomial con pesos",
  Precisión           = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 5]), 0, mconfusion_svm_polynomial_e$byClass[1, 5]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 5]), 0, mconfusion_svm_polynomial_e$byClass[2, 5]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 5]), 0, mconfusion_svm_polynomial_e$byClass[3, 5]) ) / 3, 3),
  Sensibilidad        = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 1]), 0, mconfusion_svm_polynomial_e$byClass[1, 1]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 1]), 0, mconfusion_svm_polynomial_e$byClass[2, 1]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 1]), 0, mconfusion_svm_polynomial_e$byClass[3, 1]) ) / 3, 3),
  Especificidad       = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 2]), 0, mconfusion_svm_polynomial_e$byClass[1, 2]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 2]), 0, mconfusion_svm_polynomial_e$byClass[2, 2]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 2]), 0, mconfusion_svm_polynomial_e$byClass[3, 2]) ) / 3, 3),
  kappa               = round(mconfusion_svm_polynomial_e[["overall"]][["Kappa"]], 3),
  F1                  = round((ifelse(is.na(mconfusion_svm_polynomial_e$byClass[1, 7]), 0, mconfusion_svm_polynomial_e$byClass[1, 7]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[2, 7]), 0, mconfusion_svm_polynomial_e$byClass[2, 7]) + 
                              ifelse(is.na(mconfusion_svm_polynomial_e$byClass[3, 7]), 0, mconfusion_svm_polynomial_e$byClass[3, 7])  ) / 3, 3),
  Error_clasificación = round((1 - mconfusion_svm_polynomial_e[["overall"]][["Accuracy"]]) * 100, 3),
  Accuracy            = round(mconfusion_svm_polynomial_e[["overall"]][["Accuracy"]], 3)
)

resultados_svm_polynomial_pesos

```


#### 4.7.2.3 Tabla resumen del rendimiento de los distintos modelos equilibrando pesos sin SMOTE
```{r}

tabla_resumen_equilibrado_pesos = rbind(resultados_RF_rf,
                                        #resultados_RF_pesos,
                                        #resultados_xgboost,
                                        resultados_svm_lineal_pesos,
                                        resultados_svm_radial_pesos,
                                        resultados_svm_polynomial_pesos
                                        #resultados_glmnet
                                        )

#Establecemos el orden por valores de Sensibilidad
orden = order(tabla_resumen_equilibrado_pesos$Sensibilidad, decreasing = TRUE)
tabla_resumen_equilibrado_pesos_ordenada <- tabla_resumen_equilibrado_pesos[orden, ]

kable(tabla_resumen_equilibrado_pesos_ordenada, digits = 3, caption = "Rendimiento de los modelos ajustando los pesos")
```



# 5. Conclusión

Los mejores resultados se consiguen con la validación 10 fold-crossvalidation, balanceando los datos con SMOTE y en este caso con los parámetros de fit tuning por defecto del paquete caret, los resultados se muestran en la siguiente tabla, ordenada en función de la sensibilidad, que es lo que buscamos en este estudio, una menos tasa de falsos negativos.

```{r}
kable(tabla_final, digits = 3, caption = "Rendimiento de los modelos")
```

Podemos observar que el mejor modelo es el de Random Forest, seguido de Bagging y redes neuronales. En cuanto a la precisión no se consigue por poco el objetivo de 0.80 de  precisión, pero el accuracy sí que lo supera. 

# 6. Exportación del modelo y preprocesamiento 

Para poder utilizar en la web app
```{r}
#Exportar lo datos necesarios para la web app 
save(modelo_forest, preProcValues, file="datos_ferropenia.RData")
```



